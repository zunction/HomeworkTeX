\documentclass[a4paper,10pt]{article}
\setlength{\parindent}{0cm}
\usepackage{amsmath, amssymb, amsthm, mathtools,pgfplots}
\usepackage{graphicx,caption}
\usepackage{verbatim}
\usepackage{venndiagram}
\usepackage[cm]{fullpage}
\usepackage{fancyhdr}
\usepackage{tikz}
\usepackage{listings}
\usepackage{color,enumerate,framed}
\usepackage{color,hyperref}
\definecolor{darkblue}{rgb}{0.0,0.0,0.5}
\hypersetup{colorlinks,breaklinks,
            linkcolor=darkblue,urlcolor=darkblue,
            anchorcolor=darkblue,citecolor=darkblue}

%\usepackage{tgadventor}
%\usepackage[nohug]{diagrams}
\usepackage[T1]{fontenc}
%\usepackage{helvet}
%\renewcommand{\familydefault}{\sfdefault}
%\usepackage{parskip}
%\usepackage{picins} %for \parpic.
%\newtheorem*{notation}{Notation}
%\newtheorem{example}{Example}[section]
%\newtheorem*{problem}{Problem}
\theoremstyle{definition}
%\newtheorem{theorem}{Theorem}
%\newtheorem*{solution}{Solution}
%\newtheorem*{definition}{Definition}
%\newtheorem{lemma}[theorem]{Lemma}
%\newtheorem{corollary}[theorem]{Corollary}
%\newtheorem{proposition}[theorem]{Proposition}
%\newtheorem*{remark}{Remark}
%\setcounter{section}{1}

\newtheorem{thm}{Theorem}[section]
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{defn}[thm]{Definition}
\newtheorem*{examp}{Example}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{rmk}[thm]{Remark}
\newtheorem*{nte}{Note}
\newtheorem*{notat}{Notation}

%\diagramstyle[labelstyle=\scriptstyle]

\lstset{frame=tb,
  language=Oz,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}


\pagestyle{fancy}




\fancyhead{}
\renewcommand{\headrulewidth}{0pt}

\lfoot{\color{black!60}{\sffamily Zhangsheng Lai}}
\cfoot{\color{black!60}{\sffamily Last modified: \today}}
\rfoot{\color{black!60}{\sffamily\thepage}}



\begin{document}
\flushright{Zhangsheng Lai\\1002554}
\section*{Statistics: Homework 1}

\begin{enumerate}
\item[1.19] 
\begin{enumerate}
\item Let $X_1, X_2$ and $X_3$ denote the computer owners that use Macintosh, Windows and Linux respectively and let $V$ denote the event that the user's system is infected with the virus. We want to find $\mathbb{P}(X_2 | V)$
\begin{align*}
\mathbb{P}(X_2 | V) &= \frac{\mathbb{P}(V|X_2)\mathbb{P}(X_2)}{\sum_{i=1}^{3}\mathbb{P}(V|X_i)\mathbb{P}(X_i)}\\
&=\frac{(.82)(.5)}{(.65)(.3)+(.82)(.5)+(.5)(.2)}\\
&=0.581560284
\end{align*}
%\item Since the event that the first person has a system that is infected by a virus and the second person has a system that is infected by a virus are independent events,
\item $\mathbb{P}(V) = (.65)(.3)+(.82)(.5)+(.5)(.2) = .705 $
\item Let $A$ and $B$ denote the event that the second person has a system that was also infected by a virus and the second person is known to have the same computer system as the first person. We observe that $A$ and $B$ are independent events as the probability of getting a virus on your computer system is the same regardless of whether the second person has the same computer system as the first person. Thus $\mathbb{P}(A|B) = \mathbb{P}(A) = \mathbb{P}(V) = .705$

\end{enumerate}
\item[2.4] 
\begin{enumerate}[(a)]
\item 
\begin{align*}
%F_X(x) := \begin{cases}
%\int_{0}^{x}1/4\,dy & 0 < x < 1\\
%\int_{0}^{1}1/4\,dy + \int_{3}^{x}3/8\,dy & 3 < x < 5\\
%0 & \text{otherwise}\\
%\end{cases}\\
F_X(x):= \begin{cases}
\frac{1}{4}x & 0 < x < 1\\
\frac{1}{4} & 1 \leq x \leq 3\\
\frac{3}{8}x-\frac{7}{8} & 3 < x < 5\\
1 & x>5\\
\end{cases}
\end{align*}
\item
\begin{align*}
\mathbb{P}(Y \leq y) &= \mathbb{P}(1/X\leq y)\\
&=\mathbb{P}(X\geq 1/y)\\
&=1 - \mathbb{P}(X\leq 1/y)\\
\end{align*}
From (a):
\begin{align*}
F_Y(y)&:= \begin{cases}
\frac{15}{8}-\frac{3}{8y} & 1/5 < y < 1/3\\
\frac{3}{4} & 1/3\leq y \leq 1\\
1 - \frac{1}{4y} & y > 1\\
\end{cases}\\
f_Y(y)&:= \begin{cases}
\frac{3}{8y^2} & 1/5 < y < 1/3\\
%0 & 1/3\leq y \leq 1\\
\frac{1}{4y^2} & y >1\\
0 & \text{otherwise}\\
\end{cases}
\end{align*}
%Alternatively, we can do it in this way:

\end{enumerate}
\item[2.11]
\begin{enumerate}[(a)]
\item We see that $\mathbb{P}(X = 1) = p = \mathbb{P}(Y=0)$. Since the state space contains  $\{H,T\}$, we have $1 - \mathbb{P}(X =1,Y=0) = 1- p = \mathbb{P}(X = 0, Y = 1)$. But since
\begin{align*}
\mathbb{P}(X = 1)\mathbb{P}(Y=0) = p^2 \neq p = \mathbb{P}(X = 1, Y =0)
\end{align*}
$X$ and $Y$ are dependent.
\item By total law of probability, 
\begin{align*}
\mathbb{P}(X=x) &= \sum_{n = x}^{\infty} \mathbb{P}(X=x|N=n)\cdot \mathbb{P}(N=n)\\
&= \sum_{n = x}^{\infty} \binom{n}{x}p^x(1-p)^{n-x}
\cdot e^{-\lambda}\frac{\lambda^n}{n!}\\
&= e^{-\lambda}\frac{(\lambda p)^x}{x!}\sum_{n = x}^{\infty} \frac{[\lambda(1-p)]^{n-x}}{(n-x)!}\\
&= e^{-\lambda p}\frac{(\lambda p)^x}{x!}\\
\end{align*}
in a similar fashion, we have
\begin{align*}
\mathbb{P}(Y=y) = e^{-\lambda (1-p)}\frac{(\lambda (1-p))^y}{y!}
\end{align*}
for the joint distribution of $X$ and $Y$, 
\begin{align*}
\mathbb{P}(X=x, Y=n-x) &= \mathbb{P}(X=x, Y=n-x| N=n)\cdot \mathbb{P}(N=n)\\
&=\binom{n}{x}p^x(1-p)^{n-x}\cdot e^{-\lambda}\frac{\lambda^n}{n!}
\end{align*}
now
\begin{align*}
\mathbb{P}(X=x)\cdot\mathbb{P}(Y=y) &= e^{-\lambda p}\frac{(\lambda p)^x}{x!} \cdot e^{-\lambda (1-p)}\frac{(\lambda (1-p))^y}{y!}\\
&=\binom{x+y}{x} p^x(1-p)^y e^{-\lambda}\frac{\lambda^{x+y}}{(x+y)!} = \mathbb{P}(X=x, Y=y)
\end{align*}
which shows that $X$ and $Y$ are independent.
\end{enumerate}

\item[3.4] Let $Y_i$ denote the jump of the particle at the $i$th unit. Then $X_n=\sum_{i=1}^nY_i$. The $Y_i$'s are iid, with $\mathbb{E}(Y_i)=1-2p$ and $\mathbb{V}(Y_i) = 1 - (1-2p)^2 = 4p(1-p)$ for $i=1,2,\ldots, n$.
\begin{align*}
\mathbb{E}(X_n) &= \sum_{i=1}^n\mathbb{E}(Y_i) = n(1-2p)\\
\mathbb{V}(X_n) &= \sum_{i=1}^n\mathbb{V}(Y_i) = n\cdot4p(1-p)
\end{align*}
\item[4.3] Using Chebyshev's and Hoeffding's inequality we have
\begin{align*}
\mathbb{P}(|\overline{X}_n - p|>\epsilon) &\leq \frac{1}{4n \epsilon^2} \\
\mathbb{P}(|\overline{X}_n - p|>\epsilon) &\leq 2e^{-2n\epsilon^2}
\end{align*}
\begin{align*}
\lim_{n\to\infty}\frac{e^{-2n\epsilon^2}}{1/(4n\epsilon^2)}&=4\epsilon^2\lim_{n\to\infty}\frac{n}{e^{2n\epsilon^2}}\to 0 \text{ as } n \to \infty\\
\end{align*}
thus $\frac{1}{4n\epsilon^2}$ grows faster than $e^{-2n\epsilon^2}$ for sufficiently large $n$.
%The inequality $1+x \leq e^{x}$ for $x>0$, thus:
%\begin{align*}
%1 + x &\leq e^x\\
%1 + 2n\epsilon^2 &\leq e^{2n\epsilon^2}\\
%e^{-2n\epsilon^2} &\leq \frac{1}{1+2n\epsilon^2}\\
%2e^{-2n\epsilon^2} &<\frac{1}{n\epsilon^2}
%\end{align*}
%when $n >1/6\epsilon^2$, we have 
%\begin{align*}
%\frac{1}{8n\epsilon^2} < \frac{1}{2n\epsilon^2+1}
%\end{align*}
\item[5.7] 
\begin{enumerate}[(a)]
\item 
\begin{align*}
\mathbb{P}(|X_n-0|> \epsilon) = \mathbb{P}(X_n^2> \epsilon^2)&\leq \frac{\mathbb{E}(X_n^2)}{\epsilon^2} \text{ by Markov's inequality}\\
&=\left(\frac{1}{n}+\frac{1}{n^2}\right)\cdot\frac{1}{\epsilon^2} \to 0 \text{ as } n \to \infty
\end{align*}
\item Let $Y_n$ be as given in the question. Then we can show $Y_n \leadsto Y$ and $\mathbb{P}(Y=0)=1$ which implies $Y_n \overset{P}{\to} Y$. The cdf of $Y$, $F(t)=1$ for all $t\geq0$ and 0 otherwise.
\begin{align*}
F_n(t) &= \mathbb{P}(Y_n\leq t) = \sum_{k=0}^{\lfloor \frac{t}{n}\rfloor}e^{-1/n}\frac{(1/n)^k}{k!}\\
\lim_{n\to \infty} F_n(t) &= \lim_{n\to \infty}\sum_{k=0}^{\lfloor \frac{t}{n}\rfloor}e^{-1/n}\frac{(1/n)^k}{k!}=1
\end{align*}
therefore $Y_n \overset{P}{\to} 0$.
\end{enumerate}
%We first note that 
%\begin{align*}
%\mathbb{V}\left(n^{-1}\sum_{i=1}^{n}X_i^2-p\right) & = n^{-2}\sum_{i=1}^{n}\mathbb{V}(X_i^2) = \frac{p(1-p)}{n}
%\end{align*}
%thus for any given $\epsilon >0$,
%\begin{align*}
%\mathbb{P}\left(|n^{-1}\sum_{i=1}^{n}X_i^2-p|>\epsilon\right)\leq \frac{\mathbb{V}(n^{-1}\sum_{i=1}^{n}X_i^2-p)}{\epsilon^2}=\frac{p(1-p)}{n\epsilon^2} \to 0 \text{ as } n \to \infty
%\end{align*}
%which proves the convergence in probability. We now prove its convergence in quadratic mean:
%\begin{align}
%\mathbb{E}\left(\left[n^{-1}\sum_{i=1}^{n}X_i^2-p \right]^2\right) = \mathbb{E}\left(n^{-2}\left[\sum_{i=1}^{n}X_i^2\right]^2\right)-2\,\mathbb{E}\left(p/n\sum_{i=1}^{n}X_i^2\right)+\mathbb{E}\left(p^2\right)\label{eq}
%\end{align}
%simplifying the first term, we get
%\begin{align*}
%\mathbb{E}\left(n^{-2}\left[\sum_{i=1}^{n}X_i^2\right]^2\right) &= n^{-2}\left[\mathbb{E}\left(\sum_{i=1}^{n}X_i^4\right)+2\,\mathbb{E}\left(\sum_{i\neq j}X_i^2X_j^2\right)\right]\\
%&= n^{-2}\left[\mathbb{E}\left(\sum_{i=1}^{n}X_i^4\right)+2\,\sum_{i\neq j}\mathbb{E}\left(X_i^2\right)\,\mathbb{E}\left(X_j^2\right)\right]\\
%&= n^{-2}\left[np +2\binom{n}{2}p^2\right]\\
%&=\frac{p}{n} + \frac{n-1}{n}p^2
%\end{align*}
%the simplification of the last 2 terms in (\ref{eq}) gives $-p^2$. Thus
%\begin{align*}
%\mathbb{E}\left(\left[n^{-1}\sum_{i=1}^{n}X_i^2-p \right]^2\right) = \frac{p-p^2}{n} \to 0 \text{ as } n \to \infty
%\end{align*}
%which concludes the prove of convergence in quadratic mean.
\end{enumerate}

\end{document}