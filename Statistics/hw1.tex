\documentclass[a4paper,10pt]{article}
\setlength{\parindent}{0cm}
\usepackage{amsmath, amssymb, amsthm, mathtools,pgfplots}
\usepackage{graphicx,caption}
\usepackage{verbatim}
\usepackage{venndiagram}
\usepackage[cm]{fullpage}
\usepackage{fancyhdr}
\usepackage{tikz}
\usepackage{listings}
\usepackage{color,enumerate,framed}
\usepackage{color,hyperref}
\definecolor{darkblue}{rgb}{0.0,0.0,0.5}
\hypersetup{colorlinks,breaklinks,
            linkcolor=darkblue,urlcolor=darkblue,
            anchorcolor=darkblue,citecolor=darkblue}

%\usepackage{tgadventor}
%\usepackage[nohug]{diagrams}
\usepackage[T1]{fontenc}
%\usepackage{helvet}
%\renewcommand{\familydefault}{\sfdefault}
%\usepackage{parskip}
%\usepackage{picins} %for \parpic.
%\newtheorem*{notation}{Notation}
%\newtheorem{example}{Example}[section]
%\newtheorem*{problem}{Problem}
\theoremstyle{definition}
%\newtheorem{theorem}{Theorem}
%\newtheorem*{solution}{Solution}
%\newtheorem*{definition}{Definition}
%\newtheorem{lemma}[theorem]{Lemma}
%\newtheorem{corollary}[theorem]{Corollary}
%\newtheorem{proposition}[theorem]{Proposition}
%\newtheorem*{remark}{Remark}
%\setcounter{section}{1}

\newtheorem{thm}{Theorem}[section]
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{defn}[thm]{Definition}
\newtheorem*{examp}{Example}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{rmk}[thm]{Remark}
\newtheorem*{nte}{Note}
\newtheorem*{notat}{Notation}

%\diagramstyle[labelstyle=\scriptstyle]

\lstset{frame=tb,
  language=Oz,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}


\pagestyle{fancy}




\fancyhead{}
\renewcommand{\headrulewidth}{0pt}

\lfoot{\color{black!60}{\sffamily Zhangsheng Lai}}
\cfoot{\color{black!60}{\sffamily Last modified: \today}}
\rfoot{\color{black!60}{\sffamily\thepage}}



\begin{document}
\flushright{Zhangsheng Lai\\1002554}
\section*{Statistics: Homework 1}

\begin{enumerate}
\item[1.19] Let $X_1, X_2$ and $X_3$ denote the computer owners that use Macintosh, Windows and Linux respectively and let $V$ denote the event that the user's system is infected with the virus. We want to find $\mathbb{P}(X_2 | V)$
\begin{align*}
\mathbb{P}(X_2 | V) &= \frac{\mathbb{P}(V|X_2)\mathbb{P}(X_2)}{\sum_{i=1}^{3}\mathbb{P}(V|X_i)\mathbb{P}(X_i)}\\
&=\frac{(.82)(.5)}{(.65)(.3)+(.82)(.5)+(.5)(.2)}\\
&=0.581560284
\end{align*}
\item[2.4] 
\begin{enumerate}[(a)]
\item 
\begin{align*}
%F_X(x) := \begin{cases}
%\int_{0}^{x}1/4\,dy & 0 < x < 1\\
%\int_{0}^{1}1/4\,dy + \int_{3}^{x}3/8\,dy & 3 < x < 5\\
%0 & \text{otherwise}\\
%\end{cases}\\
F_X(x):= \begin{cases}
\frac{1}{4}x & 0 < x < 1\\
\frac{3}{8}x-\frac{7}{8} & 3 < x < 5\\
1 & x > 5\\
\end{cases}
\end{align*}
\item
\begin{align*}
\mathbb{P}(Y \leq y) &= \mathbb{P}(1/X\leq y)\\
&=\mathbb{P}(X\geq 1/y)\\
&=1 - \mathbb{P}(X\leq 1/y)\\
\end{align*}
From (a):
\begin{align*}
F_Y(y)&:= \begin{cases}
\frac{15}{8}-\frac{3}{8y} & 1/5 < y < 1/3\\
1 - \frac{1}{4y} & y \geq 1\\
0 & \text{otherwise}\\
\end{cases}\\
f_Y(y)&:= \begin{cases}
\frac{3}{8y^2} & 1/5 < y < 1/3\\
\frac{1}{4y^2} & y \geq 1\\
0 & \text{otherwise}\\
\end{cases}
\end{align*}

\end{enumerate}
\item[2.11]
\begin{enumerate}[(a)]
\item We see that $\mathbb{P}(X = 1) = p = \mathbb{P}(Y=0)$. Since the state space contains  $\{H,T\}$, we have $1 - \mathbb{P}(X =1,Y=0) = 1- p = \mathbb{P}(X = 0, Y = 1)$. But since
\begin{align*}
\mathbb{P}(X = 1)\mathbb{P}(Y=0) = p^2 \neq p = \mathbb{P}(X = 1, Y =0)
\end{align*}
$X$ and $Y$ are dependent.
\item By total law of probability, 
\begin{align*}
\mathbb{P}(X=x) &= \sum_{n = x}^{\infty} \mathbb{P}(X=x|N=n)\cdot \mathbb{P}(N=n)\\
&= \sum_{n = x}^{\infty} \binom{n}{x}p^x(1-p)^{n-x}
\cdot e^{-\lambda}\frac{\lambda^n}{n!}\\
&= e^{-\lambda}\frac{(\lambda p)^x}{x!}\sum_{n = x}^{\infty} \frac{[\lambda(1-p)]^{n-x}}{(n-x)!}\\
&= e^{-\lambda p}\frac{(\lambda p)^x}{x!}\\
\end{align*}
in a similar fashion, we have
\begin{align*}
\mathbb{P}(Y=y) = e^{-\lambda (1-p)}\frac{(\lambda (1-p))^y}{y!}
\end{align*}
for the joint distribution of $X$ and $Y$, 
\begin{align*}
\mathbb{P}(X=x, Y=n-x) &= \mathbb{P}(X=x, Y=n-x| N=n)\cdot \mathbb{P}(N=n)\\
&=\binom{n}{x}p^x(1-p)^{n-x}\cdot e^{-\lambda}\frac{\lambda^n}{n!}
\end{align*}
now
\begin{align*}
\mathbb{P}(X=x)\cdot\mathbb{P}(Y=y) &= e^{-\lambda p}\frac{(\lambda p)^x}{x!} \cdot e^{-\lambda (1-p)}\frac{(\lambda (1-p))^y}{y!}\\
&=\binom{x+y}{x} p^x(1-p)^y e^{-\lambda}\frac{\lambda^{x+y}}{(x+y)!}
\end{align*}
which shows that $X$ and $Y$ are independent.
\end{enumerate}

\item[3.4] Let $Y_i$ denote the jump of the particle at the $i$th unit. Then $X_n=\sum_{i=1}^nY_i$. The $Y_i$'s are iid, with $\mathbb{E}(Y_i)=1-2p$ and $\mathbb{V}(Y_i) = 1 - (1-2p)^2 = 4p(1-p)$ for $i=1,2,\ldots, n$.
\begin{align*}
\mathbb{E}(X_n) &= \sum_{i=1}^n\mathbb{E}(Y_i) = n(1-2p)\\
\mathbb{V}(X_n) &= \sum_{i=1}^n\mathbb{V}(Y_i) = n\cdot4p(1-p)
\end{align*}
\item[4.3] Using Chebyshev's and Hoeffding's inequality we have
\begin{align*}
\mathbb{P}(|\overline{X}_n - p|>\epsilon) &\leq \frac{1}{4n \epsilon^2} \\
\mathbb{P}(|\overline{X}_n - p|>\epsilon) &\leq 2e^{-2n\epsilon^2}
\end{align*}
The inequality $(1+x)^r \leq e^{rx}$ for $r>0, x>0$, thus for $r=1$
\begin{align*}
x &< 1 + x \leq e^x\\
1/x &> e^{-x}\\
\frac{1}{2n\epsilon^2} &> e^{-2n\epsilon^2}\\
\frac{1}{n\epsilon^2} &> 2e^{-2n\epsilon^2}\\
\end{align*}
\item[5.7] We first note that 
\begin{align*}
\mathbb{V}\left(n^{-1}\sum_{i=1}^{n}X_i^2-p\right) & = n^{-2}\sum_{i=1}^{n}\mathbb{V}(X_i^2) = \frac{p(1-p)}{n}
\end{align*}
thus for any given $\epsilon >0$,
\begin{align*}
\mathbb{P}\left(|n^{-1}\sum_{i=1}^{n}X_i^2-p|>\epsilon\right)\leq \frac{\mathbb{V}(n^{-1}\sum_{i=1}^{n}X_i^2-p)}{\epsilon^2}=\frac{p(1-p)}{n\epsilon^2} \to 0 \text{ as } n \to \infty
\end{align*}
which proves the convergence in probability. We now prove its convergence in quadratic mean:
\begin{align}
\mathbb{E}\left(\left[n^{-1}\sum_{i=1}^{n}X_i^2-p \right]^2\right) = \mathbb{E}\left(n^{-2}\left[\sum_{i=1}^{n}X_i^2\right]^2\right)-2\,\mathbb{E}\left(p/n\sum_{i=1}^{n}X_i^2\right)+\mathbb{E}\left(p^2\right)\label{eq}
\end{align}
simplifying the first term, we get
\begin{align*}
\mathbb{E}\left(n^{-2}\left[\sum_{i=1}^{n}X_i^2\right]^2\right) &= n^{-2}\left[\mathbb{E}\left(\sum_{i=1}^{n}X_i^4\right)+2\,\mathbb{E}\left(\sum_{i\neq j}X_i^2X_j^2\right)\right]\\
&= n^{-2}\left[\mathbb{E}\left(\sum_{i=1}^{n}X_i^4\right)+2\,\sum_{i\neq j}\mathbb{E}\left(X_i^2\right)\,\mathbb{E}\left(X_j^2\right)\right]\\
&= n^{-2}\left[np +2\binom{n}{2}p^2\right]\\
&=\frac{p}{n} + \frac{n-1}{n}p^2
\end{align*}
the simplification of the last 2 terms in (\ref{eq}) gives $-p^2$. Thus
\begin{align*}
\mathbb{E}\left(\left[n^{-1}\sum_{i=1}^{n}X_i^2-p \right]^2\right) = \frac{p-p^2}{n} \to 0 \text{ as } n \to \infty
\end{align*}
which concludes the prove of convergence in quadratic mean.
\end{enumerate}

\end{document}