\documentclass[a4paper,10pt]{article}
\setlength{\parindent}{0cm}
\usepackage{amsmath, amssymb, amsthm, mathtools,pgfplots}
\usepackage{graphicx,caption}
\usepackage{verbatim}
\usepackage{venndiagram}
\usepackage[cm]{fullpage}
\usepackage{bbm}
\usepackage{fancyhdr}
\usepackage{tikz}
\usepackage{listings}
\usepackage{color,enumerate,framed}
\usepackage{color,hyperref}
\definecolor{darkblue}{rgb}{0.0,0.0,0.5}
\hypersetup{colorlinks,breaklinks,
            linkcolor=darkblue,urlcolor=darkblue,
            anchorcolor=darkblue,citecolor=darkblue}
\usepackage[utf8]{inputenc}

% Default fixed font does not support bold face
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{10} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{10}  % for normal

% Custom colors
\usepackage{color}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}

\usepackage{listings}

% Python style for highlighting
\newcommand\pythonstyle{\lstset{
language=Python,
basicstyle=\ttm,
otherkeywords={self},             % Add keywords here
keywordstyle=\ttb\color{deepblue},
emph={MyClass,__init__},          % Custom highlighting
emphstyle=\ttb\color{deepred},    % Custom highlighting style
stringstyle=\color{deepgreen},
frame=tb,                         % Any extra options here
showstringspaces=false,            % 
commentstyle=\fontseries{lc}\color{gray}
}}


% Python environment
\lstnewenvironment{python}[1][]
{
\pythonstyle
\lstset{#1}
}
{}

% Python for external files
\newcommand\pythonexternal[2][]{{
\pythonstyle
\lstinputlisting[#1]{#2}}}

% Python for inline
\newcommand\pythoninline[1]{{\pythonstyle\lstinline!#1!}}
%\usepackage{tgadventor}
%\usepackage[nohug]{diagrams}
\usepackage[T1]{fontenc}
%\usepackage{helvet}
%\renewcommand{\familydefault}{\sfdefault}
%\usepackage{parskip}
%\usepackage{picins} %for \parpic.
%\newtheorem*{notation}{Notation}
%\newtheorem{example}{Example}[section]
%\newtheorem*{problem}{Problem}
\theoremstyle{definition}
%\newtheorem{theorem}{Theorem}
%\newtheorem*{solution}{Solution}
%\newtheorem*{definition}{Definition}
%\newtheorem{lemma}[theorem]{Lemma}
%\newtheorem{corollary}[theorem]{Corollary}
%\newtheorem{proposition}[theorem]{Proposition}
%\newtheorem*{remark}{Remark}
%\setcounter{section}{1}

\newtheorem{thm}{Theorem}[section]
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{defn}[thm]{Definition}
\newtheorem*{examp}{Example}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{rmk}[thm]{Remark}
\newtheorem*{nte}{Note}
\newtheorem*{notat}{Notation}

%\diagramstyle[labelstyle=\scriptstyle]

\lstset{frame=tb,
  language=Oz,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}


\pagestyle{fancy}




\fancyhead{}
\renewcommand{\headrulewidth}{0pt}

\lfoot{\color{black!60}{\sffamily Zhangsheng Lai}}
\cfoot{\color{black!60}{\sffamily Last modified: \today}}
\rfoot{\color{black!60}{\sffamily\thepage}}



\begin{document}
\flushright{Zhangsheng Lai\\1002554}
\section*{Statistics: Homework 4}

\begin{enumerate}
\item 


\begin{enumerate}[(a)]
\item 
Given $p_i$ and $q_i$ denote the probability of choosing box 1 and 2 respectively if the ball color chosen is $i$ where $i=\{B, W ,G\}$, denoting the three different colors.  With the given information of the number of different color balls in the different boxes,
\begin{alignat*}{3}
\mathbb{P}(B|1)=4/10&\quad \mathbb{P}(B|2)=3/10&\quad \mathbb{P}(B|3)=2/10\\
\mathbb{P}(W|1)=2/10&\quad \mathbb{P}(W|2)=6/10&\quad \mathbb{P}(W|3)=0\\
\mathbb{P}(G|1)=4/10&\quad \mathbb{P}(G|2)=1/10&\quad \mathbb{P}(G|3)=8/10\\
\end{alignat*}
The risk function is represented by,
\begin{align*}
R(\theta,\hat{\theta}_{p,q}) &= \mathbb{E}_{\theta}(|\theta^2-\hat{\theta}_{p,q}|^2)\\
&=\mathbb{E}_{\theta}\left(\sum_{i\in\{B,W,G\}}L(\theta,\hat{\theta}_{p,q}(i))\mathbb{P}(i|\theta)\right)
\end{align*}
where $L(\theta,\hat{\theta}_{p,q}(i))=L(\theta,1)p_i+L(\theta,2)q_i+L(\theta,3)(1-p_i-q_i)$.Therefore,
\begin{align*}
R(1,\hat{\theta}_{p,q}) &= [q_B+4(1-p_B-q_B)]\frac{4}{10}+[q_W+4(1-p_W-q_W)]\frac{2}{10}+[q_G+4(1-p_G-q_G)]\frac{4}{10}\\
R(2,\hat{\theta}_{p,q}) &= [9p_B+4q_B+49(1-p_B-q_B)]\frac{3}{10}+[9p_W+4q_W+49(1-p_W-q_W)]\frac{6}{10}\\
&+[9p_G+4q_G+49(1-p_G-q_G)]\frac{1}{10}
%+[9p_B+4q_B+49(1-p_B-q_B)]\frac{2}{10}
\end{align*}
\item Bayes risk is given by
\begin{align*}
r(f,\theta)=\int R(\theta,\hat{\theta}_{p,q})f(\theta)\,d\theta
\end{align*}
but since our scenario is discrete, we instead have
\begin{align*}
r(f,\theta) &= \sum_{\theta=1,2}R(\theta,\hat{\theta}_{p,q})\mathbb{P}(\theta)\\
&= \lambda R(1,\hat{\theta}_{p,q})+ (1-\lambda)R(2,\hat{\theta}_{p,q})
\end{align*}
where $R(1,\hat{\theta}_{p,q})$ and $R(2,\hat{\theta}_{p,q})$ are the values are from (a).
\item Given $\lambda = 1/2$, we have
\begin{align*}
r(f,\theta)=\frac{1}{2} \left(R(1,\hat{\theta}_{p,q})+ R(2,\hat{\theta}_{p,q})\right) &=\frac{1}{2}\left(16.3-13.6p_B-14.7q_B-24.8p_W-27.6q_W-5.6p_G-5.6q_G\right)
\end{align*}
thus to the infimum of Bayes risk is when $q_B = q_W = q_G =1$.
\end{enumerate}

\item


\begin{lstlisting}[language=R,commentstyle=\fontseries{lc}\color{gray}]
library(leaps)
library(dplyr)

# Read csv file into dataframe car
car <- read.csv('carmpgdat.csv')
\end{lstlisting}


\item
\begin{lstlisting}[language=R,commentstyle=\fontseries{lc}\color{gray}]
library(dplyr)
library(magrittr)


# Reading data with separator tab
raw_riasec <- read.csv('RIASEC.csv',sep = '\t')

# (a)CLEANING UP
# List of realistic traits
realistic_trait <- c('R1','R2','R3','R4','R5','R6','R7','R8')

# Extracting out the realistic traits
raw_realistic <- raw_riasec[,realistic_trait]

# Removing rows with -1 from the dataframe
realistic <- raw_realistic %>%
  filter(R1* R2 * R3 * R4 * R5 * R6 * R7 * R8 > 0)
  

# (b)MODEL SELECTION

# Computing the score for the R trait
realistic <- realistic %>%
  rowwise() %>%
  mutate(Rscore = mean(c(R1, R2, R3, R4, R5, R6, R7, R8))) 

# Generating the training and validation set 
tr_realistic <- realistic[1:6500,]
val_realistic <- realistic[-(1:6500),]

# Building the linear model
lm_riasec <- lm(Rscore~R1, data = tr_realistic)
avg_RSS_tr = mean(lm_riasec$residuals^2)

# estimated regression function and residual sum of squares
print(lm$coefficients)
# R1 = 1.0011862 + 0.4282061 * R1


# (c)VALIDATION
reg.fn <- function(x) 1.0011862 + 0.4282061 * x

val_realistic %<>%
  mutate(pred_Rscore = reg.fn(R1),
         residuals = reg.fn(R1) - Rscore )

avg_RSS_val = mean(val_realistic$residuals^2)

print (avg_RSS_tr) # 0.4540176
print (avg_RSS_val) # 0.5376852

# The residual sum of squares for the validation set using the regression function
# is larger than the residual sum of square for the training set but are of the
# same order. Thus the model generalizes well.



\end{lstlisting}
\item
\begin{enumerate}[(a)]
\item 
%\begin{align*}
%I = \int_{1}^{2}f(x|1.5,2.3) \,dx= \frac{1}{N}\sum_{i=1}^{N}f(X_i|1.5,2.3)
%\end{align*}

\begin{lstlisting}[language=R,commentstyle=\fontseries{lc}\color{gray}]
# (a) Using Monte Carlo method with N samples

N <- 100000
I <- function(u){X = rgamma(1.5,1/2.3,n = N); return(sum(1<X & X<2)/N)}
I_hat <- I

# I_hat = 0.20146

# Estimated standard error by resampling Monte Carlo 10000 times.
se_mc <- sqrt(var(sapply(1:N,I)))

# se_mc =   0.001272072

# Estimated standard error using 10000 bootstrap samples.
B <- 10000
base_sample <- rgamma(1.5,1/2.3,n = N)
I_bootstrap <- function(X){X = sample(base_sample, N, replace = TRUE);return(sum(1<X & X<2)/N)}
se_bootstrap <- sqrt(var(sapply(1:B,I_bootstrap)))

# se_bootstrap = 0.001273952

\end{lstlisting}
%\begin{python}
%def mc_integrate(alpha, beta, N = 100000):
%    sample = np.random.uniform(1,2, size = (1, N))
%    return np.mean([gamma.pdf(x, alpha, loc = 0, scale = beta) for x in sample])   
%mc_integrate(1.5, 2.3)    
%\end{python}
%Estimate $I$ using Monte Carlo method is 0.20449041416849226.
%\begin{python}
%# Empirical distribution to draw bootstrap samples
%N = 100000
%sample = np.random.uniform(1,2, size = (1, N))
%emp_dist = [gamma.pdf(x, 1.5, loc = 0, scale = 2.3) for x in sample]
%# Creating the 10000 bootstrap samples
%bs_samples = [np.random.choice(emp_dist[0], 100000) for x in range(10000)]
%# Getting an estimate of I for each bootstrap sample
%bs_estimates = [np.mean(bs_samples[i]) for i in range(10000)]
%# Computing the standard error obtained using bootstrap method
%mean_bs_estimates = np.mean(bs_estimates)
%bs_se = np.mean([(mean_bs_estimates - bs_estimates[i])**2 for i in range(10000)])
%
%# Using the Monte Carlo method
%# Resampling the distribution 10000 times 
%mc_estimates = [mc_integrate(1.5, 2.3) for i in range(10000)]
%# Computing the standard error obtained from MC method
%mean_mc_estimates = np.mean(mc_estimates)
%mc_se = np.mean([(mean_mc_estimates - mc_estimates[i])**2 for i in range(10000)])
%\end{python}
%Bootstrap method gives a standard error of 3.23599044314e-10 and MC method gives a standard error of 3.32551091852e-10.
\item 
We see that we can rewrite $I$ as the following,
\begin{align*}
I = \int_{1}^{2}f(x|1.5,2.3)\,dx = \int_{0}^{\infty}h(x)f(x|1.5,2.3)\,dx
\end{align*}
where $h(x) = \mathbbm{1}(1\leq x \leq 2)$, an indicator function that is 1 when $1 \leq x \leq 2$ and 0 otherwise. Thus we can use the following estimator $\hat{I}$ for $I$ where $X_i\sim \text{Gamma}(1.5,2.3)$,
\begin{align*}
\hat{I} = \frac{1}{N}\sum_{i=1}^{N}h(X_i)
\end{align*}
Hence by viewing $h(X)\sim\text{Bernoulli}(p)$, where $p = \mathbb{P}(1\leq X \leq 2)$ for $X\sim \text{Gamma}(1.5,2.3)$ the standard error is given by 
\begin{align*}
Var(\hat{I}) = \frac{p(1-p)}{N}
\end{align*}
where we sourced for the value of $p$ from the website thus getting
\begin{align*}
\mathbb{P}(1\leq X \leq 2)=\int_{0}^{2}f(x|1.5,2.3)\,dx-\int_{0}^{1}f(x|1.5,2.3)\,dx=0.37173-0.16723
\end{align*}
Evaluating the value of the standard error we get $\text{\sffamily{se}} =$ 0.00127545972
%
%The standard error of $\hat{I}$ which is the estimated $I$ value using Monte Carlo method where $X_i \sim U(1,2)$ is given by
%\begin{align*}
%Var(\hat{I})&=Var\left(\frac{1}{N}\sum_{i=1}^{N}f(X_i|1.5,2.3)\right)\\
%&=\frac{1}{N}Var\left(f(X|1.5,2.3)\right), ~~\text{since the $X_i$'s are iid}
%\end{align*}
%Thus we need the first and second moments of $Y= f(X|1.5,2.3)$ where $X \sim U(1,2)$
%\begin{align*}
%\mathbb{E}\left(Y\right) & = \int_{1}^{2}f(x|1.5,2.3)\,dx = \int_{0}^{2}f(x|1.5,2.3)\,dx-\int_{0}^{1}f(x|1.5,2.3)\,dx=0.37173-0.16723\\
%\mathbb{E}\left(Y^2\right) & = \int_{1}^{2}f(x|1.5,2.3)^2\,dx
%\end{align*}
\end{enumerate}
\end{enumerate}

\end{document}