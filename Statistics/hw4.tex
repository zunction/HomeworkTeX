\documentclass[a4paper,10pt]{article}
\setlength{\parindent}{0cm}
\usepackage{amsmath, amssymb, amsthm, mathtools,pgfplots}
\usepackage{graphicx,caption}
\usepackage{verbatim}
\usepackage{venndiagram}
\usepackage[cm]{fullpage}
\usepackage{bbm}
\usepackage{fancyhdr}
\usepackage{tikz}
\usepackage{listings}
\usepackage{color,enumerate,framed}
\usepackage{color,hyperref}
\definecolor{darkblue}{rgb}{0.0,0.0,0.5}
\hypersetup{colorlinks,breaklinks,
            linkcolor=darkblue,urlcolor=darkblue,
            anchorcolor=darkblue,citecolor=darkblue}
\usepackage[utf8]{inputenc}

% Default fixed font does not support bold face
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{10} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{10}  % for normal

% Custom colors
\usepackage{color}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}

\usepackage{listings}

% Python style for highlighting
\newcommand\pythonstyle{\lstset{
language=Python,
basicstyle=\ttm,
otherkeywords={self},             % Add keywords here
keywordstyle=\ttb\color{deepblue},
emph={MyClass,__init__},          % Custom highlighting
emphstyle=\ttb\color{deepred},    % Custom highlighting style
stringstyle=\color{deepgreen},
frame=tb,                         % Any extra options here
showstringspaces=false,            % 
commentstyle=\fontseries{lc}\color{gray}
}}


% Python environment
\lstnewenvironment{python}[1][]
{
\pythonstyle
\lstset{#1}
}
{}

% Python for external files
\newcommand\pythonexternal[2][]{{
\pythonstyle
\lstinputlisting[#1]{#2}}}

% Python for inline
\newcommand\pythoninline[1]{{\pythonstyle\lstinline!#1!}}
%\usepackage{tgadventor}
%\usepackage[nohug]{diagrams}
\usepackage[T1]{fontenc}
%\usepackage{helvet}
%\renewcommand{\familydefault}{\sfdefault}
%\usepackage{parskip}
%\usepackage{picins} %for \parpic.
%\newtheorem*{notation}{Notation}
%\newtheorem{example}{Example}[section]
%\newtheorem*{problem}{Problem}
\theoremstyle{definition}
%\newtheorem{theorem}{Theorem}
%\newtheorem*{solution}{Solution}
%\newtheorem*{definition}{Definition}
%\newtheorem{lemma}[theorem]{Lemma}
%\newtheorem{corollary}[theorem]{Corollary}
%\newtheorem{proposition}[theorem]{Proposition}
%\newtheorem*{remark}{Remark}
%\setcounter{section}{1}

\newtheorem{thm}{Theorem}[section]
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{defn}[thm]{Definition}
\newtheorem*{examp}{Example}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{rmk}[thm]{Remark}
\newtheorem*{nte}{Note}
\newtheorem*{notat}{Notation}

%\diagramstyle[labelstyle=\scriptstyle]

\lstset{frame=tb,
  language=Oz,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}


\pagestyle{fancy}




\fancyhead{}
\renewcommand{\headrulewidth}{0pt}

\lfoot{\color{black!60}{\sffamily Zhangsheng Lai}}
\cfoot{\color{black!60}{\sffamily Last modified: \today}}
\rfoot{\color{black!60}{\sffamily\thepage}}



\begin{document}
\flushright{Zhangsheng Lai\\1002554}
\section*{Statistics: Homework 4}

\begin{enumerate}
\item 


\begin{enumerate}[(a)]
\item 
Given $p_i$ and $q_i$ denote the probability of choosing box 1 and 2 respectively if the ball color chosen is $i$ where $i=\{B, W ,G\}$, denoting the three different colors.  With the given information of the number of different color balls in the different boxes,
\begin{alignat*}{3}
\mathbb{P}(B|1)=4/10&\quad \mathbb{P}(B|2)=2/10&\quad \mathbb{P}(B|3)=4/10\\
\mathbb{P}(W|1)=3/10&\quad \mathbb{P}(W|2)=6/10&\quad \mathbb{P}(W|3)=1/10\\
\mathbb{P}(G|1)=2/10&\quad \mathbb{P}(G|2)=0&\quad \mathbb{P}(G|3)=8/10\\
\end{alignat*}
The risk function is represented by,
\begin{align*}
R(\theta,\hat{\theta}_{p,q}) &= \mathbb{E}_{\theta}(|\theta^2-\hat{\theta}_{p,q}|^2)\\
&=\mathbb{E}_{\theta}\left(\sum_{i\in\{B,W,G\}}L(\theta,\hat{\theta}_{p,q}(i))\mathbb{P}(i|\theta)\right)
\end{align*}
where $L(\theta,\hat{\theta}_{p,q}(i))=L(\theta,1)p_i+L(\theta,2)q_i+L(\theta,3)(1-p_i-q_i)$.Therefore,
\begin{align*}
R(1,\hat{\theta}_{p,q}) &= [q_B+4(1-p_B-q_B)]\frac{4}{10}+[q_W+4(1-p_W-q_W)]\frac{3}{10}+[q_G+4(1-p_G-q_G)]\frac{2}{10}\\
R(2,\hat{\theta}_{p,q}) &= [9p_B+4q_B+49(1-p_B-q_B)]\frac{2}{10}+[9p_W+4q_W+49(1-p_W-q_W)]\frac{6}{10}%+[9p_B+4q_B+49(1-p_B-q_B)]\frac{2}{10}
\end{align*}
\item Bayes risk is given by
\begin{align*}
r(f,\theta)=\int R(\theta,\hat{\theta}_{p,q})f(\theta)\,d\theta
\end{align*}
but since our scenario is discrete, we instead have
\begin{align*}
r(f,\theta) &= \sum_{\theta=1,2}R(\theta,\hat{\theta}_{p,q})\mathbb{P}(\theta)\\
&= \lambda R(1,\hat{\theta}_{p,q})+ (1-\lambda)R(2,\hat{\theta}_{p,q})
\end{align*}
where $R(1,\hat{\theta}_{p,q})$ and $R(2,\hat{\theta}_{p,q})$ are the values are from (a).
\item Given $\lambda = 1/2$, we have
\begin{align*}
r(f,\theta)=\frac{1}{2} \left(R(1,\hat{\theta}_{p,q})+ R(2,\hat{\theta}_{p,q})\right) &=\frac{1}{20}\left(428-96p_B-102q_B-252p_W-279q_W-8p_G-6q_G\right)
\end{align*}
thus to the infimum of Bayes risk is when $q_B = q_W = p_G =1$.
\end{enumerate}

\item


\begin{lstlisting}[language=R,commentstyle=\fontseries{lc}\color{gray}]
library(leaps)
library(dplyr)

# Read csv file into dataframe car
car <- read.csv('carmpgdat.csv')
\end{lstlisting}


\item
\begin{lstlisting}[language=R,commentstyle=\fontseries{lc}\color{gray}]
library(dplyr)
library(magrittr)


# Reading data with separator tab
raw_riasec <- read.csv('RIASEC.csv',sep = '\t')

# (a)CLEANING UP
# List of realistic traits
realistic_trait <- c('R1','R2','R3','R4','R5','R6','R7','R8')

# Extracting out the realistic traits
raw_realistic <- raw_riasec[,realistic_trait]

# Removing rows with -1 from the dataframe
realistic <- raw_realistic %>%
  filter(R1* R2 * R3 * R4 * R5 * R6 * R7 * R8 > 0)
  

# (b)MODEL SELECTION

# Computing the score for the R trait
realistic <- realistic %>%
  rowwise() %>%
  mutate(Rscore = mean(c(R1, R2, R3, R4, R5, R6, R7, R8))) 

# Generating the training and validation set 
tr_realistic <- realistic[1:6500,]
val_realistic <- realistic[-(1:6500),]

# Building the linear model
lm <- lm(Rscore~R1, data = tr_realistic)
avg_RSS_tr = mean(lm$residuals^2)

# estimated regression function and residual sum of squares
print(lm$coefficients)
# R1 = 1.0011862 + 0.4282061 * R1


# (c)VALIDATION
reg.fn <- function(x) 1.0011862 + 0.4282061 * x

val_realistic %<>%
  mutate(pred_Rscore = reg.fn(R1),
         residuals = reg.fn(R1) - Rscore )

avg_RSS_val = mean(val_realistic$residuals^2)

print (avg_RSS_tr) # 0.4540176
print (avg_RSS_val) # 0.5376852

# The residual sum of squares for the validation set using the regression function
# is larger than the residual sum of square for the training set but are of the
# same order. Thus the model generalizes well.



\end{lstlisting}
\item
\begin{enumerate}[(a)]
\item Using basic Monte Carlo,
\begin{align*}
I = \int_{1}^{2}f(x|1.5,2.3) \,dx= \frac{1}{N}\sum_{i=1}^{N}f(X_i|1.5,2.3)
\end{align*}
\begin{python}
def mc_integrate(alpha, beta, N = 100000):
    sample = np.random.uniform(1,2, size = (1, N))
    return np.mean([gamma.pdf(x, alpha, loc = 0, scale = beta) for x in sample])   
mc_integrate(1.5, 2.3)    
\end{python}
Estimate $I$ using Monte Carlo method is 0.20449041416849226.
\begin{python}
# Empirical distribution to draw bootstrap samples
N = 100000
sample = np.random.uniform(1,2, size = (1, N))
emp_dist = [gamma.pdf(x, 1.5, loc = 0, scale = 2.3) for x in sample]
# Creating the 10000 bootstrap samples
bs_samples = [np.random.choice(emp_dist[0], 100000) for x in range(10000)]
# Getting an estimate of I for each bootstrap sample
bs_estimates = [np.mean(bs_samples[i]) for i in range(10000)]
# Computing the standard error obtained using bootstrap method
mean_bs_estimates = np.mean(bs_estimates)
bs_se = np.mean([(mean_bs_estimates - bs_estimates[i])**2 for i in range(10000)])

# Using the Monte Carlo method
# Resampling the distribution 10000 times 
mc_estimates = [mc_integrate(1.5, 2.3) for i in range(10000)]
# Computing the standard error obtained from MC method
mean_mc_estimates = np.mean(mc_estimates)
mc_se = np.mean([(mean_mc_estimates - mc_estimates[i])**2 for i in range(10000)])
\end{python}
Bootstrap method gives a standard error of 3.23599044314e-10 and MC method gives a standard error of 3.32551091852e-10.
\item The standard error of 
\end{enumerate}
\end{enumerate}

\end{document}