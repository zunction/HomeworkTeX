
\begin{flushright}
Date: 011116
\end{flushright}

\begin{thm}
Consider a $(\lambda,\mu)-$cost minimization game with a positive potential function $\Phi$ such that $\Phi(\mathbf{s}) \leq cost(\mathbf{s})$ for every outcome $\mathbf{s}$. Let $\mathbf{s^0},\mathbf{s^1},\ldots,\mathbf{s^T}$ be a sequence generated by MaxGain best response dynamics, $\mathbf{s^\ast}$ a minimum cost outcome and $1>\gamma>0$ is a parameter, Then for all but 
\begin{align}
%O\left(
\frac{k}{\gamma(1-\mu)}\log\frac{\Phi(\mathbf{s^0})}{\Phi_{min}}\label{eq:outcomes}
%\right)
\end{align}
outcomes $\mathbf{s}^t$ satisfy
%\begin{align*}
%cost(\mathbf{s^t})\leq \left(\frac{\lambda}{1-\mu}+\frac{1}{1-\gamma}\right)\cdot cost(\mathbf{s^\ast})
%\end{align*}
\begin{align}
%cost(\mathbf{s^t})\leq \left(\frac{\lambda}{1-\mu}+\gamma\right)\cdot cost(\mathbf{s^\ast})
cost(\mathbf{s^t})\leq \left(\frac{\lambda}{(1-\mu)(1-\gamma)}\right)\cdot cost(\mathbf{s^\ast})\, \label{eq:good}
\end{align}
\begin{proof}
\begin{align}
cost(\mathbf{s^t})&\leq \sum_i c_i(\mathbf{s^t})\notag\\
&=\sum_i\left[c_i(s_i^\ast,s_{-i}^t)+\delta_i(\mathbf{s^t})\right],\quad \delta_i(\mathbf{s^t})=c_i(\mathbf{s^t})-c_i(s_i^\ast,s^t_{-i})\notag\\
&\leq \lambda \cdot cost(\mathbf{s^\ast})+\mu \cdot cost(\mathbf{s^t})+\sum_i\delta_i(\mathbf{s^t})\notag\\
cost(\mathbf{s^t})&\leq \frac{\lambda}{1-\mu}\cdot cost(\mathbf{s^\ast}) + \frac{1}{1-\mu}\cdot \sum_i\delta_i(\mathbf{s^t}) \label{eq:2}
\end{align}
we shall let $\Delta(\mathbf{s^t})=\sum_i\delta_i(\mathbf{s^t})$ in the remaining parts of the proof. We shall now define a state $\mathbf{s^t}$ to be bad if it does not satisfy (\ref{eq:good}) and 
%\begin{align}
%cost(\mathbf{s^t})>\left(\frac{\lambda}{1-\mu}+\frac{1}{1-\gamma}\right)\cdot cost(\mathbf{s^t})
%\end{align} 
%\begin{align*}
%cost(\mathbf{s^t})> \left(\frac{\lambda}{1-\mu}+\gamma\right)\cdot cost(\mathbf{s^\ast})
%\end{align*}
by (\ref{eq:2}), when $\mathbf{s^t}$ is bad we get
\begin{align*}
\Delta(\mathbf{s^t})&\geq \gamma(1-\mu)\cdot cost(\mathbf{s^t})\\
\end{align*}
By the MaxGain definition and the inequality relating the potential function and cost,
\begin{align*}
\max_{i}\delta_i(\mathbf{s^t})\geq \frac{\Delta(\mathbf{s^t})}{k}\geq \frac{\gamma(1-\mu)}{k}\cdot cost(\mathbf{s^t})\geq \frac{\gamma(1-\mu)}{k}\cdot \Phi(\mathbf{s^t})
\end{align*}
and we get what we desire as
\begin{align*}
\Phi(\mathbf{s^t})-\Phi(s_i^\ast,s^t_{-i})
=c_i(\mathbf{s^t})-c_i(s_i^\ast,s^t_{-i})&=\delta_i(\mathbf{s^t})
\end{align*}
and hence
\begin{align}
%\left(1-\frac{\gamma(1-\mu)}{k}\right)\Phi(\mathbf{s^t})\geq \Phi(s_i^\ast,s^t_{-i})
\left(1-\frac{\gamma(1-\mu)}{k}\right)\Phi(\mathbf{s^t})\geq \Phi(\mathbf{s^{t+1}})\label{eq:result}
\end{align}
whenever $\mathbf{s^t}$ is a bad state. The equation in (\ref{eq:result}) says that for every MaxGain best response dynamics, if the state is bad, the new state $\mathbf{s^{t+1}}$ is smaller than the previous state $\mathbf{s^t}$ by a factor of $1-\frac{\gamma(1-\mu)}{k}$. By Lemma \ref{lemma:exp}, the potential decreases by a factor of $e$ for every $\frac{k}{\gamma(1-\mu)}$ bad states encountered. Thus solving 
\begin{align*}
e^{-n}\Phi(\mathbf{s^0}) \geq \Phi_{min}
\end{align*}
shows (\ref{eq:outcomes}).
\end{proof}
\end{thm}

\section{No Regret Learning}
Consider a set $A$ of actions with $|A|=n$, then at time $t=1,2,\ldots, T$
\begin{itemize}
\item A decision maker picks a mixed strategy $p^t$ (i.i. a probability distribution function over its actions $A$)
\item An adversary (nature) picks a cost vector $c^t:A\to [0,1]$
\item An action $a^t$ is chosen accordingly to the distribution $p^t$ and the decision maker incurs cost $c^t(a^t)$. The decision maker learns the entire cost vector $s^t$ and not just the realised cost.
\end{itemize}

\begin{defn}[Time Average of Regret]
The time average of regret of the action sequence $a^1,a^2,\ldots,a^T$ with respect to $a\in A$ is 
\begin{align*}
\frac{1}{T}\sum_{t=1}^{T}c^t(a^t)-\sum_{t=1}^{T}c^t(a)
\end{align*}
\end{defn}

\begin{defn}[No Regret Algorithm]
Let $\mathcal{A}$ be an online decision making algorithm.
\begin{enumerate}[(a)]
\item An adversary for $\mathcal{A}$ is a function that takes an input the day $t$, the history of mixed strategies $p^1,p^2,\ldots,p^t$ produced by $\mathcal{A}$ on the first $t$ days and the realised actions $a^1,a^2,\ldots, a^{t-1}$ of the first $t-1$ days and outputs a cost vector $c^t:A\to [0,1]$.
\item An online decision making algorithm has no (external) regret if for every adversary, the expected regret with respect to every action $a\in A$ converges to $0$ as $T\to \infty$.
\begin{align*}
R=\frac{1}{t}\left(\sum_{t}c^t(a^t)-\min_{a \in A}\sum_{t}c^t(a)\right)\leq \mathcal{O}(1)
\end{align*}
\end{enumerate}
\end{defn}



