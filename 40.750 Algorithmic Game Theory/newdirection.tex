\documentclass[a4paper,10pt]{article}
\setlength{\parindent}{0cm}
\usepackage{amsmath, amssymb, amsthm, mathtools,pgfplots}
\usepackage{graphicx,caption}
\usepackage{verbatim}
\usepackage{venndiagram}
\usepackage[cm]{fullpage}
\usepackage{fancyhdr}
\usepackage{tikz}
\usepackage{listings,url,}
\usepackage{color,enumerate,framed}
\usepackage{color,hyperref}
\definecolor{darkblue}{rgb}{0.0,0.0,0.5}
\hypersetup{colorlinks,breaklinks,
            linkcolor=darkblue,urlcolor=darkblue,
            anchorcolor=darkblue,citecolor=darkblue}
\usetikzlibrary{arrows,positioning} 
\usepackage{sectsty}
%\allsectionsfont{\centering}
%\usepackage[normalem]{ulem}
%\allsectionsfont{\sffamily}
%\sectionfont{\centering\ulemheading{\uuline}}

%\usepackage{tgadventor}
%\usepackage[nohug]{diagrams}
\usepackage[T1]{fontenc}
%\usepackage{helvet}
%\renewcommand{\familydefault}{\sfdefault}
\usepackage{parskip}
%\usepackage{picins} %for \parpic.
%\newtheorem*{notation}{Notation}
%\newtheorem{example}{Example}[section]
%\newtheorem*{problem}{Problem}
\theoremstyle{definition}
%\newtheorem{theorem}{Theorem}
%\newtheorem*{solution}{Solution}
%\newtheorem*{definition}{Definition}
%\newtheorem{lemma}[theorem]{Lemma}
%\newtheorem{corollary}[theorem]{Corollary}
%\newtheorem{proposition}[theorem]{Proposition}
%\newtheorem*{remark}{Remark}
%\setcounter{section}{1}

\newtheorem{thm}{Theorem}[section]
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{defn}[thm]{Definition}
\newtheorem*{examp}{Example}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{rmk}[thm]{Remark}
\newtheorem*{nte}{Note}
\newtheorem*{notat}{Notation}

%\diagramstyle[labelstyle=\scriptstyle]

\lstset{frame=tb,
  language=Oz,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}


\pagestyle{fancy}




\fancyhead{}
\renewcommand{\headrulewidth}{0pt}

\lfoot{\color{black!60}{\sffamily Zhangsheng}}
\cfoot{}
\cfoot{\color{black!60}{\sffamily Last modified: \today}}
\rfoot{\color{black!60}{\sffamily\thepage}}



\begin{document}
\begin{flushright}
Zhangsheng Lai
\end{flushright}

\section*{A new direction}
We noticed that in most of the models that we read (cf.  \cite{Drago1991}, \cite{Chakravarti2015}), the workplace reward maximizing game that was played is static and there are no dynamics involved. However, in real scenarios, when someone joins a new company, the newcomer does not immediately decide to purely cooperate or compete, similiarly, this is true for his new colleagues too. After some interaction with each other over a time span, everyone will have a rough idea of everybody's work style, ethnics and most importantly the reward derived. This will then allow the agent to decide whether to cooperate or compete with each agent respectively which we shall call it the nature of the agent. Such a model would be a more realistic scenario as it gives each agent time to learn about the environment before determining his nature. 

\begin{defn}
The \emph{nature} of an agent in a workplace reward maximizing game with $k$ agents is a vector $\mathbf{x_i}=(x^i_1,x^i_2,\ldots, x^i_k)$ such that $\sum x_i=1$. It is a discrete probability distribution where $x^i_j$ denotes the probability of cooperating with agent $j$ and $x^i_i$ denotes the probability of not cooperating. \label{def:nature}
\end{defn}

With the definition \ref{def:nature} we see that a \emph{completely cooperative} agent $i$ is one with $x^i_i=0$ and a \emph{completely competitive} agent has $x^i_i=1$. A game where all the agents are completely cooperative is called a \emph{completely cooperative environment} and if all the agents are completely competitive, it is a \emph{completely competitive environment}.

We would now like to introduce some form of dynamics in which the players learn whether it is to their benefit to cooperate or compete with a given agent. Here we will approach it in a similar fashion of \emph{no-regret dynamics} where we want to minimize regret in single decision-maker playing game against an adversary. In our setting the adversary is the other $k-1$ agents and its ability to extend it to multi-player games and relation to coarse correlated equilibria is how we plan to model our game. (We would then vary the different reward systems to see how it will cause the CCE to change and whether we can design a reward system with the dominant-strategy incentive-compatible (DSIC) for workplace reward maximizing games.)

\subsection*{The model} 
Let $A$ be a set of actions playable by the agent in the workplace reward maximizing game with $|A|\geq 2$. At time $t=1,2,\ldots, T$
\begin{enumerate}[(i)]
\item The agent plays a mixed strategy $p^t$, which is a probability distribution over $A$.
\item The adversary picks a utility (cost) vector $u^t: A \to [0,1]$.\footnote{The other $k-1$ agents models the adversary well: when an agent $i$ plays his strategy, other agents might play strategies that might possibly increase his cost which nicely simulates that the adversary picks a cost vector $u^t$ after the agent $i$ picks his mixed strategy.}
\item An action $u^t$ is then chosen according to the distribution $p^t$ and the decision-maker incurs a cost of $u^t(a^t)$. The decision-maker learns the entire utility (cost) vector $u^t$ and not just the realized cost $u^t(a^t)$.\footnote{The learning of the entire cost vector is also reasonable in real-life scenarios as often after making a wrong decision, you will think of what if you played the other strategy that is more beneficial for you.}
\end{enumerate}

The actions of the workplace reward maximizing game is which colleague to work with. Hence in a workplace with $n$ agents, there will be $n$ possible actions for each agent; $n-1$ of them denote working with each of the other agent and the last being working by ownself. % This in a workplace with $$
As in the definitions in no-regret dynamics we define regret and the no-regret algorithm the same way.

\begin{defn}[Time-averaged regret]
The \emph{time-averaged regret} of the action sequencce $a^1,a^2,\ldots, a^T$ with respect to a fixed action $a$ is 
\begin{align*}
\frac{1}{T}\sum_{t=1}^{T}\left[u^t(a)-u^t(a^t)\right]%\left[c^t(a^t)-c^t(a)\right] 
\end{align*}	
Note: since the adversary will want to minimise your utility, the action that you play at each time $t$ will give a smaller utility than playing a constant action.
\end{defn}

\begin{defn}[No-Regret Algorithm]
Let $\mathcal{A}$ be an online decision-making algorithm.
\begin{enumerate}[(a)]
\item The adversary for $\mathcal{A}$ is a function that takes as input the day $t$, the mixed strategies $p^1,p^2,\ldots,p^t$ produced by $\mathcal{A}$ on the first $t$ days and the realized actions, $a^1,a^2,\ldots, a^{t-1}$ of the first $t-1$ days and produces as output a utility (cost) vector $u^t:A\to [0,1]$.
\item An online decision-making algorithm has \emph{no external regret} if for every adversary for it, the expected regret with respect to every action $a\in A$ is $\mathcal{O}(1)$ as $T \to \infty$.
\end{enumerate}
\end{defn}
The interpretation of the output of the utilty function must be explained more explicitly; the naive interpretation for now will be for a reward $R_n$ obtained for a particular task $n$, the amount of reward shared with you will be $R_n(u^t(a_n))$ where $a_n$ denotes the action of choosing task $n$.


\subsection*{The Algorithm}
As we are measuring welfare using the agent's utility the \emph{multiplicative weights} (MW) algorithm has to be modified accordingly for it to work with utility, that is the weights of each action increases as the rate of increase depends on the utility gained from an action.
\begin{enumerate}[1.]
\item Initialize $w^1(a)=1$ for all $a \in A$
\item For $t=1,2,\ldots,T$
\begin{enumerate}[(a)]
\item Choose a task to work on according to a distribution $p^t:=w^t/\Gamma^t$, where $\Gamma^t=\sum_{a\in A}w^t(a)$ is the sum of the weights.
\item For a given utility vector, increase the weights using the formula $w^{t+1}(a)=w^t(a)\cdot (1+\epsilon)^{u^t(a)}$ for every action $a \in A$.
\end{enumerate}
Thus, if the proportion of the reward that is allocated to you is larger for working with a particular agent, then in the next time step, a larger weight is assigned for that agent.
\end{enumerate}


\subsection{Two Agent Scenario}
Consisder two agents each with a project on its own. Let agent $i$ has strategy $p^t_{ij}$: probability of joining project $j$. At each time step (which we shall more precisely define later), the project produces a reward $R_j^t$ to be spilt among agents in that project such that
\begin{align*}
R_j^t=\sum_{i\in R}u_{ij}^t
\end{align*} 
Agent $i$ update his weights with every time step such that for all $i, j$ $\sum_ip_{ij}^t=1$. This is done by the multiplicative weights (MW) update algorithm. For every time step the reward is calculated by the following in a two agent scenario
\begin{align*}
R_{i}(\mathbf{p})
\end{align*}
and the agent's utility can be computed using the matrix
\begin{align*}
\begin{pmatrix}
R_{1} &R_{2}\\
\end{pmatrix}
\begin{pmatrix}
p_{11} & p_{12}\\
p_{21} & p_{22}\\
\end{pmatrix}
\begin{pmatrix}
R_{1}\\
R_{2}\\
\end{pmatrix}
\end{align*}


















































\bibliography{AGT_project} 
\bibliographystyle{apalike}
\end{document}