\begin{flushright}
Date: 081116
\end{flushright}


In the earlier lesson, the idea of the correlated equilibrium was introduced to be that for every player $i \in \{1,2,\ldots ,k\}$, strategy $s_i\in S_i$ and every deviation $s'_i$ 
\begin{align*}
\mathbf{E}_{s \sim \sigma}[c_i(s_i,s_{-i})\mid s_i] \leq \mathbf{E}_{s \sim \sigma}[c_i(s_i',s_{-i})\mid s_i]
\end{align*}
for a distribution $\sigma$ on $S_1 \times \ldots \times S_k$. We shall now present an equivalent definition of CE in next definition.


\begin{defn}[Correlated Equilibrium]
A correlated equilibrium of a cost minimization game is a distribution $\sigma$ over outcomes such that for every player $i$ with strategy set $S_i$ and every switching function $\delta: S_i \to S_i$
\begin{align*}
\mathbf{E}_{s \sim \sigma}[c_i(s_i,s_{-i})] \leq \mathbf{E}_{s \sim \sigma}[c_i(\delta(s_i),s_{-i})]
\end{align*}
\end{defn}


The switching function is more explicitly defined as 
\begin{align*}
\delta(x_i):=\begin{cases}
s'_i, &\text{ if input is $s_i$}\\
x_i, & \text{ otherwise}
\end{cases}
\end{align*}
and so $\delta$ is simply saying which advice I would follow and what I would do if I don't follow; captures the obedience of the player.  

\begin{defn}
An online decision making algorithm as no swap regret if for every adversary for it, the expected swap regret
\begin{align*}
\frac{1}{T}\left[\sum_{t=1}^{T}c^t(a^t)-\sum_{t=1}^{T}c^t(\delta(a^t))\right]
\end{align*}
with respect to every switching function $\delta:A \to A$ is $o(1)$ as $T \to\infty$.
\end{defn}
In the average time of regret definition introduced earlier, it corresponds to a subset of the delta functions, in particularly the constant delta functions as the strategy played every day that is based on a probability distribution is compared against a fixed action. Hence an algorithm with no swap regret is also an algorithm with no external regret.
\begin{thm}
Suppose after $T$ iterations of no-swap-regret dynamics, every player of a cost minimization game has swap regret at most $\epsilon$ for each of its switching functions. Let $\sigma^t=\prod_{i=1}^{k}p_i^t$ denote the outcome distribution at time $t$ and $\sigma = \frac{1}{T}\sum_{t=1}^{T}\sigma^t$ the time average history of these distributions. Then $\sigma$ is an $\epsilon-$approximate equilibrium 
\begin{align*}
\mathbf{E}_{s \sim \sigma}[c_i(s_i,s_{-i})] \leq \mathbf{E}_{s \sim \sigma}[c_i(\delta(s_i),s_{-i})]
\end{align*}
for every player $i$ and switching function $\delta:S_i\to S_i$.
\end{thm}


\begin{thm}
If there is a no-external-regret algorithm, then there is a no-swap-regret algorithm.
\begin{proof}
Let $n$ be the number of actions and let $M_1, \ldots M_n$ are different no external regret algorithms. At $t=1,2, \ldots, T$:
\begin{itemize}
\item Receive distributions $q^t_1,\ldots ,q^t_n$ from the no regret algorithms $M_1, \ldots M_n$.
\item Compute and output an consensus algorithm $p^t$
\item Receive a cost vector $c^t$ from the adversary.
\item Give algorithm $M_j$ the cost vector $p^t(j)\cdot c^t$.
\end{itemize}

\begin{figure}[h]
\caption{Blackbox to be drawn...}
\end{figure}

The expected cost of the master algorithm is 
\begin{align}
\frac{1}{T}\sum_{t=1}^{T}\sum_{i=1}^{n}p^t(i)c^t(i) \label{eq:master}
\end{align}
where $i$ is the indicator of the $i$th action out of $n$. The time average expected cost under a switching function $\delta: A \to A$ is 
\begin{align}
\frac{1}{T}\sum_{t=1}^{T}\sum_{i=1}^{n}p^t(i)c^t(\delta(i))\label{eq:switching}
\end{align}
We would like to show that (\ref{eq:master}) is upper bounded by (\ref{eq:switching}). Looking at a specific $M_j$ algorithm, which believes that the incoming costs are of the form $p^1(j)\cdot c^1,p^1(2)\cdot c^2, \ldots, p^T(j)\cdot c^T$ where the superscript are referencing to the day and produces probability distributions $q^1_j, q^2_j,\ldots,q^T_j$. Thus the algorithm $M_j$ perceives its expected cost as 
\begin{align}
\frac{1}{T}\sum_{t=1}^{T}\sum_{i=1}^{n}q^t_j(i)[p^t(j)c^t(i)]\label{eq:algoj}
\end{align}
and we understand $q^t_j(i)$ as the probability of $M_j$ choosing action $i$ and $p^t(j)c^t(i)$ the cost experienced when choosing action $i$. As $M_j$ is a no regret algorithm, its cost for a fixed action $k\in A$ is at most 
\begin{align}
\frac{1}{T}\sum_{t=1}^{T}p^t(j)c^t(k)+R_j\label{eq:fixedk}
\end{align}
where $R_j \to 0$ as $T \to \infty$. Fix a switching function $\delta$. Summing the (\ref{eq:algoj}) and (\ref{eq:fixedk}) with $k$ instantiated as $\delta(j)$ in (\ref{eq:fixedk}) over the $n$ no regret algorithms, we see that the second summation is an upper bound for the first.
\begin{align}
\frac{1}{T}\sum_{t=1}^{T}\sum_{i=1}^{n}\sum_{j=1}^{n}q^t_j(i)[p^t(j)c^t(i)]\leq \frac{1}{T}\sum_{t=1}^{T}\sum_{j=1}^{n}p^t(j)c^t(k)+\sum_{j=1}^{n}R_j \label{eq:algojfixedkinequality}
\end{align}
In the inequality above, we know that $\sum_{j=1}^{n}R_j \to 0$ as $T \to \infty$. Observe that if we can choose the consensus distribution $p^1,\ldots, p^T$ so that (\ref{eq:master}) and the left hand side of (\ref{eq:algojfixedkinequality}) coincide, we are golden. We show how to choose $p^t$ such that for each $i \in A$ and $t=1,2,\ldots, T$
\begin{align*}
p^t(i)=\sum_{j=1}^{n}q^t_j(i)p^t(j)
\end{align*}
Upon closer inspection of  the equation above, we might find the relation similar to that of a stationary distribution of a Markov chain. The key idea is given distributions $q^t_1,\ldots, q^t_n$ from the algorithms $M_1, \ldots, M_n$ form the transition matrix $W$ with transition probabilities $w_{ij}=q^t_j(i)$. A probability distribution $p^t$ satisfies the above equation iff it is the stationary distribution of this Markov chain. Such a distribution exists and can be found in polynomial time using eigenvector solvers.
\end{proof}

\end{thm}




