{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revenue Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider the general case where the length of the buffer is $n$. Then the states are given by $\\mathcal{X}:=A \\cup B$, where $A= \\{0,\\ldots, n\\}$ and $B= \\{n+1,\\ldots, 2n+1\\}$, with $A$ denoting the states where the server is off and $B$ denoting the states where the server is on. The number of customers in the queue is exactly the state number for states in $A$ and the number of customers in the queue for states in $B$ is modulo $n+1$ of the state number. As for the action, we have $\\mathcal{A}(x) = \\{0,1\\}$ where 0 (1) means the server is off (on).\n",
    "\n",
    "For our exercise, we have `n=100`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, we can get evaluate the reward function by considering cases.\n",
    "\n",
    "#### Server is switched off, $a = 0$\n",
    "\n",
    "- $x <n$, $R(x,a,w) = (1-w)(-x)+w(-x-1)$\n",
    "- $x =n$, $R(x,a,w) = (1-w)(-n)+w(-n-1000)$\n",
    "- $n< x < 2n+1$, $R(x,a,w) = (1-w)(-(x \\mod n+1))+w(-(x \\mod n+1)-1)$\n",
    "- $x =2n+1$, $R(x,a,w) = (1-w)(-n)+w(-n-1000)$\n",
    "\n",
    "#### Server is switched off, $a = 1$\n",
    "\n",
    "- $x <n$, $R(x,a,w) = (1-w)(-x)+w(-x-1)-10$\n",
    "- $x =n$, $R(x,a,w) = (1-w)(-n)+w(-n-1)-10$\n",
    "- $n< x < 2n+1$, $R(x,a,w) = (1-w)(-(x \\mod n+1))+w(-(x \\mod n+1)-1)$\n",
    "- $x =2n+1$, $R(x,a,w) = (1-w)(-n)+w(-n-1)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm, inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reward_function(x,a,w = 0.75,n = 100):\n",
    "    \"\"\"\n",
    "    Computes the expected reward given current state x, action taken a, weight w and buffer length n.\n",
    "    \"\"\"\n",
    "    if a == 0:\n",
    "        if (x < n):\n",
    "            r = (1-w)*(-x) + w*(-x-1)\n",
    "        elif ((x == n) or (x == 2*n+1)):\n",
    "            r = (1-w)*(-n) + w*(-n-1000);\n",
    "        elif (x > n) and (x < (2*n+1) ):\n",
    "            y = np.mod(x,n+1)\n",
    "            r = (1-w)*(-y) + w*(-y-1)\n",
    "    else:        \n",
    "        if (x <= n):\n",
    "            r = (1-w)*(-x) + w*(-x-1)-10\n",
    "        elif (x > n) and (x < (2*n+1) ):\n",
    "            y = np.mod(x,n+1)\n",
    "            r = (1-w)*(-y) + w*(-y-1)\n",
    "        else: \n",
    "            r = (1-w)*(-n) + w*(-n-1)  \n",
    "        \n",
    "    return r    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `reward_gen` generates two vectors `r0` and `r1` which gives us the expected rewards when we take $a=0$ and $a=1$ respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reward_gen(w = 0.75, n = 100):\n",
    "    \"\"\"\n",
    "    Generates the rewards for action a=0 and a=1 given weight w and buffer length n.\n",
    "    \"\"\"\n",
    "    R0 = np.zeros(2*(n+1))\n",
    "    R1 = np.zeros(2*(n+1))\n",
    "    for i in range(2*(n+1)):\n",
    "        R0[i] = reward_function(i,0,w,n)\n",
    "        R1[i] = reward_function(i,1,w,n)\n",
    "        \n",
    "    return R0, R1   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate the reward vectors `r0` and `r1` here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "R0, R1 = reward_gen()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have the discount factor $\\alpha = 0.98$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = .98"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the maximum over the possible action $\\mathcal{A}=\\{0,1\\}$ over the entire state space $\\mathcal{X}$, we get the reward vector `r`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "R = np.maximum(R0,R1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def T(R, J, alpha = 0.98):\n",
    "    \"\"\"\n",
    "    Applies the T operator:\n",
    "    (TJ)(x) = max E[R(x,a,w) + alpha*J(f(x,a,w))|x]\n",
    "    where r = max R(x,a,w) over action space\n",
    "          alpha is the discount factor\n",
    "          J is expected total reward\n",
    "    \"\"\"\n",
    "    J = R + alpha*J\n",
    "    \n",
    "    return J\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialise the initial `J` to be all zeros and apply the operator `T` multiple times till the error is small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 1654\n",
      "Error: 0.0\n"
     ]
    }
   ],
   "source": [
    "J = np.zeros(2*n+1+1)\n",
    "err = 1\n",
    "k = 0\n",
    "while err > 0:\n",
    "    J = T(R, J)\n",
    "    err = norm((T(R,J)-J),1)\n",
    "    k += 1\n",
    "\n",
    "print ('Iterations:', k)    \n",
    "print ('Error:', err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above, we see that we needed 1654 iterations for $(TJ^{k})(x) = J(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would now like to derive a policy to based on the `J` values derived from value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K = J.reshape(2,101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(K,axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By comparing the the expected total reward for each state $x \\in \\mathcal{X}$ we see that the optimal policy is to keep the server off when there are less than 100 customers in the queue and to switch on the server when there are 100 customers in the queue. This policy can be explained by the huge cost incurred when a customer leaves when there is no more space in the buffer. Also the cost incurred when there are less than 100 customers in the queue is the same regardless of whether the server is on or off; the same cost in incurred whether we allow the customer to wait in the queue or when the server is switched on to serve the customer. The cost of 10 to switch on the server is also a deterance to switch on the server, unless we have a possibility of a large cost to be incurred; when there is no more space in the buffer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialise the initial stationary policy, `pi` to be switch off the server at every state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pi = np.zeros(2*(n+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transition probability matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transition probability matrix under policy `pi` is given by \n",
    "\n",
    "- if $0 \\leq i \\leq 100$\n",
    "\n",
    "$$p_{ij}(\\text{pi}(i)) =\\begin{cases} \n",
    "0.25 &  j = i, pi(i) = 0, i <100\\\\\n",
    "0.75 &  j = i+1, pi(i) = 0, i <100\\\\\n",
    "1 & j=i, p(i) = 0, i = 100\\\\\n",
    "1 & j=i + 101, p(i) = 1, i = 0\\\\\n",
    "0.25 &  j = i + 100, pi(i) = 1, i < 100\\\\\n",
    "0.75 &  j = i + 101, pi(i) = 1, i <100\\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases} $$\n",
    "\n",
    "- if $101 \\leq i \\leq 201$\n",
    "\n",
    "$$p_{ij}(\\text{pi}(i)) =\\begin{cases} \n",
    "0.25 &  j = i-101, pi(i) = 0, i <201\\\\\n",
    "0.75 &  j = i-100, pi(i) = 0, i <201\\\\\n",
    "1 & j=i-101, p(i) = 0, i = 201\\\\\n",
    "1 & j=i, p(i) = 1, i = 101\\\\\n",
    "0.25 &  j = i-1, pi(i) = 1, i < 201\\\\\n",
    "0.75 &  j = i, pi(i) = 1, i <201\\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trans_mat_gen(pi, n = 100):\n",
    "    \"\"\"\n",
    "    Generates transition probability matrix under the policy pi. The buffer is of length n.\n",
    "    \"\"\"\n",
    "    P = np.zeros((2*(n+1),2*(n+1)))\n",
    "    for i in range(n+1):\n",
    "        if pi[i] == 0:\n",
    "            P[i,i] = 0.25\n",
    "            if i < n:\n",
    "                P[i,i+1] = 0.75\n",
    "            else:\n",
    "                P[i,i] = 1\n",
    "        else:\n",
    "            P[i,i+n+1] = 0.75\n",
    "            if i > 0:\n",
    "                P[i,i+n] = 0.25\n",
    "            else:\n",
    "                P[i,i+n+1] = 1\n",
    "    \n",
    "    for i in range(n+1, 2*(n+1)):\n",
    "        if pi[i] == 0:\n",
    "            P[i,i-(n+1)] = 0.25\n",
    "            if i < (2*n+1):\n",
    "                P[i,i-n] = 0.75\n",
    "            else:\n",
    "                P[i,i-(n+1)] = 1\n",
    "        else:\n",
    "            P[i,i] = 0.75\n",
    "            if i > (n+1):\n",
    "                P[i,i-1] = 0.25\n",
    "            else:\n",
    "                P[i,i] = 1\n",
    "         \n",
    "    return P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A toy example where the buffer is 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pi1 = np.array([1,0,1,0,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  0.  ,  0.  ,  1.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.25,  0.75,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ,  0.25,  0.75],\n",
       "       [ 0.25,  0.75,  0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.25,  0.75,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ,  0.25,  0.75]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_mat_gen(pi1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected reward vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The expected reward vector under policy `pi` is given to be\n",
    "\n",
    "$$R_i = \\mathbb{E}[R(i, \\text{pi}(i), w)]$$\n",
    "\n",
    "where $i$ is the $i-$th index of the reward vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- if pi(i) = 0, which means the server is switched off\n",
    "\n",
    "    - for $0\\leq i < n$, $R_i = (1-w)(-i)+w(-i-1)$\n",
    "    - for $i = n$, $R_i = (1-w)(-n)+w(-n-1000)$\n",
    "    - for $n+1\\leq i < 2n+1$, $R_i = (1-w)(-(i-(n+1))) + w(-(i-(n+1))-1)$\n",
    "    - for $i = 2n+1$, $R_i = (1-w)(-n)+w(-n-1000)$\n",
    "\n",
    "- if pi(i) = 1, which means the server is switched on\n",
    "    - for $0\\leq i < n$, $R_i = (1-w)(-i)+w(-i-1)-10$\n",
    "    - for $i = n$, $R_i = (1-w)(-n)+w(-n-1)-10$\n",
    "    - for $n+1\\leq i < 2n+1$, $R_i = (1-w)(-(i-(n+1))) + w(-(i-(n+1))-1)$\n",
    "    - for $i = 2n+1$, $R_i = (1-w)(-n)+w(-n-1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reward_vec(pi, w = 0.75, n = 100):\n",
    "    \"\"\"\n",
    "    Takes in a vector pi that describes the policy, the weight w and returns the expected reward vector.\n",
    "    \"\"\"\n",
    "    R = np.zeros(2*(n+1))\n",
    "    for i in range(2*(n+1)):\n",
    "        if pi[i] == 0:\n",
    "            if (i < n):\n",
    "                R[i] = (1-w)*(-i) + w*(-i-1)\n",
    "            elif i == n:\n",
    "                R[i] = (1-w)*(-n) + w*(-n-1000)\n",
    "            elif (i >= (n+1)) and (i < 2*n+1):\n",
    "                R[i] = (1-w)*(-(i-(n+1))) + w*(-(i-(n+1))-1)\n",
    "            elif i == 2*n+1:\n",
    "                R[i] = (1-w)*(-n) + w*(-n-1000)    \n",
    "        else:\n",
    "            if (i < n):\n",
    "#                 R[i] = (1-w)*(-i) + w*(-i-1) - 10\n",
    "                R[i] = (1-w)*(-(i-1)) + w*(-i) - 10 - 1     \n",
    "            elif i == n:\n",
    "                R[i] = (1-w)*(-n) + w*(-n-1) - 10\n",
    "            elif (i >= (n+1)) and (i < 2*(n+1)):\n",
    "                R[i] = (1-w)*(-(i-(n+1))) + w*(-(i-(n+1))-1)\n",
    "            else:\n",
    "                R[i] = (1-w)*(-n) + w*(-n-1)\n",
    "            \n",
    "    return R                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_eval(pi, w = 0.75, alpha = 0.98, n = 100):\n",
    "    \"\"\"\n",
    "    Returns the corresponding reward function under the policy pi.\n",
    "    \"\"\"\n",
    "    P = trans_mat_gen(pi, n)\n",
    "    I = np.eye(P.shape[0])\n",
    "    R = reward_vec(pi, w = w, n = n)\n",
    "    \n",
    "    return np.dot(inv(I-alpha*P),R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_improv(pi, alpha = 0.98, w = 0.75, n = 100):\n",
    "    \"\"\"\n",
    "    Returns the new optimal policy pi based on the given transition probability matrix P (based on previous pi), \n",
    "    reward function J, discount factor alpha, weight w and buffer length n.\n",
    "    \"\"\"\n",
    "    # compute the reward for actions\n",
    "    R0, R1 = reward_gen(w = w, n = n)\n",
    "    R = np.maximum(R0,R1) \n",
    "    \n",
    "    pi0 = np.zeros(2*(n+1))\n",
    "    pi1 = np.zeros(2*(n+1)) + 1\n",
    "    \n",
    "    # compute the transition probability matrix for the actions\n",
    "    P0 = trans_mat_gen(pi0, n = n)\n",
    "    P1 = trans_mat_gen(pi1, n = n)\n",
    "        \n",
    "    # compute the expectations for the actions\n",
    "    J = policy_eval(pi, w = w, alpha = alpha, n = n)\n",
    "    E0 = R0 + alpha * np.dot(P0,J)\n",
    "    E1 = R1 + alpha * np.dot(P1,J)\n",
    "    \n",
    "    # stacking the expected rewards together and choosing the one that return better rewards\n",
    "    E = np.vstack((E0,E1))\n",
    "    new_pi = np.argmax(E, axis = 0)\n",
    "    \n",
    "    return new_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_iter(pi, alpha = 0.98, w = 0.75, n = 100):\n",
    "    \"\"\"\n",
    "    Performs policy iteration till policy reaches a stationary policy\n",
    "    \"\"\"\n",
    "    k = 0\n",
    "    while not np.array_equal(pi, policy_improv(pi, alpha = alpha, w = w, n = n)):\n",
    "        pi, E = policy_improv(pi, alpha = alpha, w = w, n = n)\n",
    "        k += 1\n",
    "        \n",
    "    print ('Iterations:', k)    \n",
    "    return pi, E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = np.array([0,0,0,0,0,0])\n",
    "n = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.32450331   1.28941713  47.38607956  -0.          -0.          -0.        ]\n",
      " [  0.           1.32450331  48.67549669   0.           0.           0.        ]\n",
      " [  0.           0.          50.           0.           0.           0.        ]\n",
      " [  0.32450331   1.28941713  47.38607956   1.           0.           0.        ]\n",
      " [  0.           0.32450331  48.67549669   0.           1.           0.        ]\n",
      " [  0.           0.          49.           0.           0.           1.        ]]\n"
     ]
    }
   ],
   "source": [
    "s = inv(np.eye(6) - .98*trans_mat_gen(t, n))\n",
    "print (s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-35637.58168501 -36606.29139073 -37600.         -35637.58168501\n",
      " -36606.29139073 -37600.        ]\n"
     ]
    }
   ],
   "source": [
    "print (np.dot(inv(np.eye(6) - .98*trans_mat_gen(t, n)),reward_vec(t, w = .75, n = 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R0: [ -7.50000000e-01  -1.75000000e+00  -7.52000000e+02  -7.50000000e-01\n",
      "  -1.75000000e+00  -7.52000000e+02]\n",
      "R1: [-10.75 -11.75 -12.75  -0.75  -1.75  -2.75]\n",
      "[ -0.75  -1.75 -12.75  -0.75  -1.75  -2.75]\n"
     ]
    }
   ],
   "source": [
    "R0, R1 = reward_gen(w = 0.75, n = 2)\n",
    "R = np.maximum(R0,R1) \n",
    "print ('R0:',R0)\n",
    "print ('R1:',R1)\n",
    "print (R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.]\n",
      "[ 1.  1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "n = 2\n",
    "t0 = np.zeros(2*(n+1))\n",
    "t1 = np.zeros(2*(n+1)) + 1\n",
    "print (t0)\n",
    "print (t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.25  0.75  0.    0.    0.    0.  ]\n",
      " [ 0.    0.25  0.75  0.    0.    0.  ]\n",
      " [ 0.    0.    1.    0.    0.    0.  ]\n",
      " [ 0.25  0.75  0.    0.    0.    0.  ]\n",
      " [ 0.    0.25  0.75  0.    0.    0.  ]\n",
      " [ 0.    0.    1.    0.    0.    0.  ]]\n",
      "[[ 0.    0.    0.    1.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.25  0.75  0.  ]\n",
      " [ 0.    0.    0.    0.    0.25  0.75]\n",
      " [ 0.    0.    0.    1.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.25  0.75  0.  ]\n",
      " [ 0.    0.    0.    0.    0.25  0.75]]\n"
     ]
    }
   ],
   "source": [
    "P0 = trans_mat_gen(t0, n = n)\n",
    "P1 = trans_mat_gen(t1, n = n)\n",
    "print (P0)\n",
    "print (P1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-35637.58168501 -36606.29139073 -37600.         -35637.58168501\n",
      " -36606.29139073 -37600.        ]\n"
     ]
    }
   ],
   "source": [
    "J = policy_eval(t, w = 0.75, alpha = 0.98, n = 2)\n",
    "print (J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-35637.58168501 -36606.29139073 -37600.         -35637.58168501\n",
      " -36606.29139073 -37600.        ]\n",
      "[-34935.58005131 -35648.58168501 -36617.29139073 -34925.58005131\n",
      " -35638.58168501 -36607.29139073]\n"
     ]
    }
   ],
   "source": [
    "E0 = R0 + alpha * np.dot(P0,J)\n",
    "E1 = R1 + alpha * np.dot(P1,J)\n",
    "print (E0)\n",
    "print (E1)\n",
    "E = np.vstack((E0,E1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.25  0.75  0.    0.    0.    0.  ]\n",
      "[-35637.58168501 -36606.29139073 -37600.         -35637.58168501\n",
      " -36606.29139073 -37600.        ]\n",
      "-36364.1139643\n"
     ]
    }
   ],
   "source": [
    "print (P0[0,:])\n",
    "print (J)\n",
    "print (np.dot(P0[0,:],J))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  1.  0.  0.]\n",
      "[-35637.58168501 -36606.29139073 -37600.         -35637.58168501\n",
      " -36606.29139073 -37600.        ]\n",
      "-35637.581685\n"
     ]
    }
   ],
   "source": [
    "print (P1[0,:])\n",
    "print (J)\n",
    "print (np.dot(P1[0,:],J))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
