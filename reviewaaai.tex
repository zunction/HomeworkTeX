\documentclass[a4paper,10pt]{article}
%\setlength{\parindent}{0cm}
\usepackage{amsmath, amssymb, amsthm, mathtools,pgfplots}
\usepackage{graphicx,caption}
\usepackage{verbatim}
\usepackage{venndiagram}
\usepackage[cm]{fullpage}
\usepackage{fancyhdr}
\usepackage{tikz}
\usepackage{listings,url,}
\usepackage{color,enumerate,framed}
\usepackage{color,hyperref}
\definecolor{darkblue}{rgb}{0.0,0.0,0.5}
\hypersetup{colorlinks,breaklinks,
            linkcolor=darkblue,urlcolor=darkblue,
            anchorcolor=darkblue,citecolor=darkblue}

\usepackage{sectsty}
\allsectionsfont{\centering}
%\usepackage[normalem]{ulem}
%\allsectionsfont{\sffamily}
%\sectionfont{\centering\ulemheading{\uuline}}

%\usepackage{tgadventor}
%\usepackage[nohug]{diagrams}
\usepackage[T1]{fontenc}
%\usepackage{helvet}
%\renewcommand{\familydefault}{\sfdefault}
%\usepackage{parskip}
%\usepackage{picins} %for \parpic.
%\newtheorem*{notation}{Notation}
%\newtheorem{example}{Example}[section]
%\newtheorem*{problem}{Problem}
\theoremstyle{definition}
%\newtheorem{theorem}{Theorem}
%\newtheorem*{solution}{Solution}
%\newtheorem*{definition}{Definition}
%\newtheorem{lemma}[theorem]{Lemma}
%\newtheorem{corollary}[theorem]{Corollary}
%\newtheorem{proposition}[theorem]{Proposition}
%\newtheorem*{remark}{Remark}
%\setcounter{section}{1}

\newtheorem{thm}{Theorem}[section]
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{defn}[thm]{Definition}
\newtheorem*{examp}{Example}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{rmk}[thm]{Remark}
\newtheorem*{nte}{Note}
\newtheorem*{notat}{Notation}

%\diagramstyle[labelstyle=\scriptstyle]

\lstset{frame=tb,
  language=Oz,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}


\pagestyle{fancy}




\fancyhead{}
\renewcommand{\headrulewidth}{0pt}

\lfoot{\color{black!60}{\sffamily Zhangsheng Lai}}
\cfoot{}
\cfoot{\color{black!60}{\sffamily Last modified: \today}}
\rfoot{\color{black!60}{\textsc{\thepage}}}



\begin{document}
%\flushright{Zhangsheng Lai\\1002554}
\section*{Review: Building Adversary Resistant Deep Neural Networks with Random Feature Nullification}
\subsection*{Summary}
The paper presents the random feature nullification (RFN) to preserve the DNN's integrity against adversarial samples. The problem is motivated by pointing out that a DNN can be tricked into doing a wrong classification by introducing perturbations that the human eye cannot observe. The perturbations are obtained using the \emph{fast gradient sign} method \cite{goodfellow2014explaining} where a cost function $\mathcal{L}(\theta,X,Y)$ is differentiated with respect to $X$ denoted by
\begin{align*}
\mathcal{J}_{\mathcal{L}}(X)=\frac{\partial \mathcal{L}(\theta,X,Y)}{\partial X}
\end{align*}
and the perturbation is calculated by the formula
\begin{align*}
\delta X = \phi \cdot sign(\mathcal{J}_{\mathcal{L}}(X))
\end{align*}


Based on the discussion on the currently related work to increase the DNN's resistance to adversarial samples, the inclusion of a non-deterministic feature is serves to make the construction of effective adversarial samples\footnote{adversarial samples that is able to successful deceive the DNN to make wrong classifications.} hard for the attacker. The trade off of the model is that every sample undergoes a nullification process before it is being trained. The nullification  is implemented by using a $I_p$ matrix with dimensions same as the real sample and each $i,j$ component to be Bernoulli distributed with parameter $p$. 


(weakness of adversarial training and model complexity enhancement here?) The paper points out that to be able to put up a strong resistance against adversarial attacks, the model architecture has to be one that makes it impossible to generate adversarial samples even when it is disclosed. However, \cite{goodfellow2014explaining} 





\subsection*{Positivity}
The discussed approaches to tackle the problem of adversarial attacks have so far been trying to train the model on adversarial samples or to increase the non-linearity of the model. The RFN technique adopts another approach which is to make the construction of effective adversarial samples a tall order for the attackers, through the addition of random variables. Making it hard to construct adversarial samples might be a better approach that future efforts of tackling adversarial attacks might want to explore as the doing adversarial training has its limitations; (the augmentation of adversarial samples to the training set to increase the resistance of the model from adversarial samples only trains the model well on adversarial samples that it has been exposed to.)





%- the introduced method is more efficient than the method of adversarial training; there is no need to train on both the real sample and a crafted adversarial sample which is an important aspect as the amout of real samples we potentially have is inexhaustive.
%
%- 


\subsection*{Negativity}

%Using $I_p$ matrices to randomly nullify the 
In its adversarial resistant analysis on why its RFN method is more robust than the known methods of adversarial resistance, its argument only assumed that the attackers used the same method to obtain perturbations which are then added to the real sample to form the adversarial samples. The discussion on how attackers might tweak their methodology to generate adversarial samples effective against them is little, not every clear and not convincing. Based on my naive thinking, the attacker might chose to not draw a sample $I_p^\ast$ which then the nullified adversarial sample looks like
\begin{align*}
(X+\delta X ) \odot I_p = (X \odot I_p) + \delta X\odot I_p
\end{align*}
thus there is no distortion of the $\delta X$. In the first place how much different is $\delta X \odot I_p$ from $\delta X$ is still not really known based on my point in the third paragraph. But the lack of analysis on the next point makes the RFN method less desirable as a technique to foil adversarial attacks.


From \cite{goodfellow2014explaining} adversarial examples are known to generalize, i.e. an adversarial example generated for one model can be misclassified by another model, although they were trained on different learning architectures using disjoint training sets. Hence although the experimental results shown by the paper does numerically points to better resilience of their RFN method, it is only resilient to adversarial samples constructed using their RFN learning technique; it say anything about resilience against adversarial samples generated from other learning techniques. 


The paper worked with only the MNIST dataset where each real sample is a $28 \times  28$ grayscale image. And the nullification of MNIST data to 0 would simply mean it changed the pixel to a white pixel and since an image from MNIST is predominantly white\footnote{no preprocessing of the dataset is assumed here as the paper does not say anything about it}, it is possible that the RFN technique does not really ``null'' much of the information from the images. A reason for this suspicion is even with their choice of nullification rate to be a high $50\%$, the classification error is 0.0170. Even with the highest reported nullification rate of $70\%$ the classification error is still  a decent 0.0266. Such results are counter-intuitively, as one might feel that by nulling such a high proportion of the image could be though of as the image being occluded and occlusions in images limit the information that could be obtained (might need to find some papers that support my claim here). Such a problem might also cause a problem in trying to extend the RFN technique to colour images like CIFAR-10, CIFAR-100 and ImageNet, which leads to the next point.

As seen in the examples from \cite{szegedy2013intriguing}, we see that the perturbations introduced are not visible to the human eye but are classified by the trained model as classes that are very far from its original class. A lack of showing how their RFN technique to datasets of colour images like those mentioned above makes this paper less complete. Perhaps the RFN technique cannot be applied to colour images or the paper has not considered extending to a more complex dataset with more classes in a bigger space. 


%slightly less attractive as compared to if it did some work on the extension of their technique to those datasets. As from the examples shared in \cite{szegedy2013intriguing} on how adversarial samples that are generated from images from ImageNet are not distinguishable to 
%
%shows that discussion on adversarial samples are not restricted to MNIST datasets.

%\subsection*{My Thoughts}
%- how does the technique to create adversarial samples work? How do we add the perturbation to make the DNN believe that it is of a particular class $A$ or $B$ when it is actually of class $C$?
%
%
%- why compute  $\frac{\partial \mathcal{L}(\theta, f(\tilde{X},I_p),Y)}{\partial \tilde{X}}$ instead of  $\frac{\partial \mathcal{L}(\theta, f(\tilde{X},I_p),Y)}{\partial  f(\tilde{X},I_p)}$? Would seeing the data as $f(\tilde{X},I_p)$  be better than seeing it as $\tilde{X}$? Or perhaps I should first myself if that derivative can be computed. (my tentative answer is yes). Although $I_p$ is a random variable, what it does is that it nullifies every sample to 0 with proportion $p$. This can be seen as a form of occlusion with the image being occluded at many small spots.
%
%- is $\delta X \odot I_p^\ast$ a notation or the exact representation of the perturbation?
%
%- can the attacker not draw a sample $I_p^\ast$ which then the nullified adversarial sample looks like
%\begin{align*}
%(X+\delta X {\color{black!30}\odot I_p^\ast}) \odot I_p = (X \odot I_p) + \delta X {\color{black!30}\odot I_p^\ast} \odot I_p
%\end{align*}
%
%- could explore deeper on how attackers can bypass the random feature nullification technique other than the traditional method used by them to produce adversarial samples.
%
%- could have tested it on CIFAR dataset since MNIST dataset is a smaller space than the CIFAR dataset and this technique might not work as well on images with more colors and variety. There are papers that the author cited from that discussed about adversarial examples in other datasets.
%
%- why is there a choice of different activation function and learning rate to train the models which makes the comparison not fair.
%
%- game theory aspects of the paper: every sample is seen as an agent and they are competing to have their cost reduced, i.e. to be classified correctly. However in the midst of this competition, the social cost of the DNN (which is the error made by the DNN) is increased as it is susceptible to adversarial attacks. Hence we want to construct the DNN such that the NE of the game is such that it is not susceptible to the adversarial samples.
%
%- does not talk about the confidence of the predictions.

%\cite{goodfellow2014explaining}
%\cite{tesla}
\newpage

\bibliography{review} 
\bibliographystyle{apalike}













\end{document}