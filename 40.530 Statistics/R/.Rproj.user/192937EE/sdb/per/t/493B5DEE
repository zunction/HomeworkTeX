{
    "collab_server" : "",
    "contents" : "library(leaps)\nlibrary(survey)\nlibrary(dplyr)\n\n# read csv file into dataframe car\ncar <- read.csv('carmpgdat.csv')\n\n# (a) Fitting a mulitple linear regression model\n\n# generate a linear model with normally distributed noise with the model MPG~VOL+HP+SP+WT\n# covariates <-cbind('VOL','HP','SP','WT')\nlm_car <- lm(MPG~VOL+HP+SP+WT, data = car)\n\n\n# estimated regression function and residual sum of squares\ncoefficients(lm_car)\n# MPG = 192.43775332 - 0.01564501 * VOL + 0.39221231 * HP - 1.29481848 * SP - 1.85980373 * WT\n\nRSS = sum(lm_car$residuals^2)\n# RSS = 1027.381\n\n# (b) Mallows's Cp\n# Since the AIC criterion is equivalent to Mallows's Cp, \n\n# (i) Forward\nbase <- lm(MPG~1, data = car)\nstep(base, scope = list(upper = lm_car, lower=~1), direction = 'forward', trace = TRUE)\n\n# Start:  AIC=378.69\n# MPG ~ 1\n# \n# Df Sum of Sq    RSS    AIC\n# + WT    1    6641.5 1466.0 240.45\n# + HP    1    5058.0 3049.4 300.51\n# + SP    1    3842.6 4264.9 328.02\n# + VOL   1    1101.6 7005.9 368.72\n# <none>              8107.5 378.69\n# \n# Step:  AIC=240.45\n# MPG ~ WT\n# \n# Df Sum of Sq    RSS    AIC\n# + SP    1    82.981 1383.0 237.68\n# + HP    1    35.380 1430.6 240.45\n# <none>              1466.0 240.45\n# + VOL   1     3.883 1462.1 242.24\n# \n# Step:  AIC=237.68\n# MPG ~ WT + SP\n# \n# Df Sum of Sq    RSS    AIC\n# + HP    1    349.37 1033.7 215.80\n# + VOL   1     45.97 1337.0 236.90\n# <none>              1383.0 237.68\n# \n# Step:  AIC=215.8\n# MPG ~ WT + SP + HP\n# \n# Df Sum of Sq    RSS   AIC\n# <none>              1033.7 215.8\n# + VOL   1    6.2685 1027.4 217.3\n# \n# Call:\n#   lm(formula = MPG ~ WT + SP + HP, data = car)\n# \n# Coefficients:\n#   (Intercept)           WT           SP           HP  \n# 194.1296      -1.9221      -1.3200       0.4052 \n\n# (ii) Backward\nstep(lm_car,direction = 'backward', trace = TRUE)\n\n# Start:  AIC=217.3\n# MPG ~ VOL + HP + SP + WT\n# \n# Df Sum of Sq    RSS    AIC\n# - VOL   1      6.27 1033.7 215.80\n# <none>              1027.4 217.30\n# - HP    1    309.67 1337.0 236.90\n# - SP    1    373.36 1400.7 240.72\n# - WT    1   1013.76 2041.2 271.59\n# \n# Step:  AIC=215.8\n# MPG ~ HP + SP + WT\n# \n# Df Sum of Sq    RSS    AIC\n# <none>              1033.7 215.80\n# - HP    1    349.37 1383.0 237.68\n# - SP    1    396.97 1430.6 240.45\n# - WT    1   1322.87 2356.5 281.37\n# \n# Call:\n#   lm(formula = MPG ~ HP + SP + WT, data = car)\n# \n# Coefficients:\n#   (Intercept)           HP           SP           WT  \n# 194.1296       0.4052      -1.3200      -1.9221  \n\n\n# For the forward stepwise approach, the base model is important\n# as it will change the outcome. For example for this, if we were\n# to start with VOL instead of any of the other covariates, we\n# will end up with also the VOL covariate. By not starting with with\n# the VOL covariate we will end up with a model without VOL which\n# corresponds to the backward stepwise approach and also the\n# Zheng-Loh model selection. As for the backward stepwise approach\n# we do not have such a problem as we start the model with all the\n# covariates and reduce it down by computing the AIC of the model\n# with different covariate missing then remove the covariate that\n# gives the smallest AIC when removed.\n\n\n\n# (c) Zheng-Loh Model Selection\n# Wald test for the covariates\n\nregTermTest(lm_car, 'VOL', null=NULL,df=Inf, method = \"Wald\")\nregTermTest(lm_car, 'HP', null=NULL,df=Inf, method = \"Wald\")\nregTermTest(lm_car, 'SP', null=NULL,df=Inf, method = \"Wald\")\nregTermTest(lm_car, 'WT', null=NULL,df=Inf, method = \"Wald\")\n\n# Wald test for VOL\n# in lm(formula = MPG ~ VOL + HP + SP + WT, data = car)\n# Chisq =  0.4698075  on  1  df: p= 0.49308 \n# \n# Wald test for HP\n# in lm(formula = MPG ~ VOL + HP + SP + WT, data = car)\n# Chisq =  23.20929  on  1  df: p= 1.4529e-06 \n# \n# Wald test for SP\n# in lm(formula = MPG ~ VOL + HP + SP + WT, data = car)\n# Chisq =  27.98266  on  1  df: p= 1.2241e-07 \n# \n# Wald test for WT\n# in lm(formula = MPG ~ VOL + HP + SP + WT, data = car)\n# Chisq =  75.97941  on  1  df: p= < 2.22e-16\n# \n# Arranging in descending order we have:\n#   WT > SP > HP > VOL\n# We can do this as the Chisq test statistic is just the square \n# of the test statistic of the Wald test.\n# \n# Let lm_j be the linear model with the jth largest Wald test statistic\n\nn <- nrow(car)\n\nlm_1 <- lm(MPG ~ WT, data = car)\njhat_1 = sum(lm_1$residuals^2) + (1 * RSS/(n-4) * log(n))\nprint (jhat_1)\n# jhat_1 = 1524.045\n\nlm_2 <- lm(MPG ~ WT + SP, data = car)\njhat_2 = sum(lm_2$residuals^2) + (2 * RSS/(n-4) * log(n))\nprint (jhat_2)\n# jhat_2 = 1499.108\n\nlm_3 <- lm(MPG ~ WT + SP + HP, data = car)\njhat_3 = sum(lm_3$residuals^2) + (3 * RSS/(n-4) * log(n)) \nprint (jhat_3)\n# jhat_3 =  1207.78\n\njhat_4 = RSS + (4 * RSS/(n-4) * log(n)) \nprint (jhat_4)\n# jhat_4 = 1259.555\n\n# Thus the Zheng-Loh model selection method will select WT, SP and HP \n# as the covariates for predicting the MPG. Comparing it to (b), we see\n# that the Zheng-Loh model selection method gives similar outcome to using\n# Mallows's Cp model forward and backward stepwise to selecting a model.",
    "created" : 1490713256761.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2032754524",
    "id" : "493B5DEE",
    "lastKnownWriteTime" : 1491380551,
    "last_content_update" : 1491380551875,
    "path" : "~/repos/HomeworkTeX/Statistics/R/q2.R",
    "project_path" : "q2.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}