\documentclass[a4paper,10pt]{article}
\setlength{\parindent}{0cm}
\usepackage{amsmath, amssymb, amsthm, mathtools,pgfplots}
\usepackage{graphicx,caption}
\usepackage{verbatim}
\usepackage{venndiagram}
\usepackage[cm]{fullpage}
\usepackage{bbm}
\usepackage{fancyhdr}
\usepackage{tikz}
\usepackage{listings}
\usepackage{color,enumerate,framed}
\usepackage{color,hyperref}
\definecolor{darkblue}{rgb}{0.0,0.0,0.5}
\hypersetup{colorlinks,breaklinks,
            linkcolor=darkblue,urlcolor=darkblue,
            anchorcolor=darkblue,citecolor=darkblue}
\usepackage[utf8]{inputenc}

% Default fixed font does not support bold face
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{10} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{10}  % for normal

% Custom colors
\usepackage{color}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}

\usepackage{listings}

% Python style for highlighting
\newcommand\pythonstyle{\lstset{
language=Python,
basicstyle=\ttm,
otherkeywords={self},             % Add keywords here
keywordstyle=\ttb\color{deepblue},
emph={MyClass,__init__},          % Custom highlighting
emphstyle=\ttb\color{deepred},    % Custom highlighting style
stringstyle=\color{deepgreen},
frame=tb,                         % Any extra options here
showstringspaces=false,            % 
commentstyle=\fontseries{lc}\color{gray}
}}


% Python environment
\lstnewenvironment{python}[1][]
{
\pythonstyle
\lstset{#1}
}
{}

% Python for external files
\newcommand\pythonexternal[2][]{{
\pythonstyle
\lstinputlisting[#1]{#2}}}

% Python for inline
\newcommand\pythoninline[1]{{\pythonstyle\lstinline!#1!}}
%\usepackage{tgadventor}
%\usepackage[nohug]{diagrams}
\usepackage[T1]{fontenc}
%\usepackage{helvet}
%\renewcommand{\familydefault}{\sfdefault}
%\usepackage{parskip}
%\usepackage{picins} %for \parpic.
%\newtheorem*{notation}{Notation}
%\newtheorem{example}{Example}[section]
%\newtheorem*{problem}{Problem}
\theoremstyle{definition}
%\newtheorem{theorem}{Theorem}
%\newtheorem*{solution}{Solution}
%\newtheorem*{definition}{Definition}
%\newtheorem{lemma}[theorem]{Lemma}
%\newtheorem{corollary}[theorem]{Corollary}
%\newtheorem{proposition}[theorem]{Proposition}
%\newtheorem*{remark}{Remark}
%\setcounter{section}{1}

\newtheorem{thm}{Theorem}[section]
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{defn}[thm]{Definition}
\newtheorem*{examp}{Example}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{rmk}[thm]{Remark}
\newtheorem*{nte}{Note}
\newtheorem*{notat}{Notation}

%\diagramstyle[labelstyle=\scriptstyle]

\lstset{frame=tb,
  language=Oz,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}


\pagestyle{fancy}




\fancyhead{}
\renewcommand{\headrulewidth}{0pt}

\lfoot{\color{black!60}{\sffamily Zhangsheng Lai}}
\cfoot{\color{black!60}{\sffamily Last modified: \today}}
\rfoot{\color{black!60}{\sffamily\thepage}}



\begin{document}
\flushright{Zhangsheng Lai\\1002554}
\section*{Statistics: Homework 4}

\begin{enumerate}
\item 


\begin{enumerate}[(a)]
\item 
Given $p_i$ and $q_i$ denote the probability of choosing box 1 and 2 respectively if the ball color chosen is $i$ where $i=\{B, W ,G\}$, denoting the three different colors.  With the given information of the number of different color balls in the different boxes,
\begin{alignat*}{3}
\mathbb{P}(B|1)=4/10&\quad \mathbb{P}(B|2)=3/10&\quad \mathbb{P}(B|3)=2/10\\
\mathbb{P}(W|1)=2/10&\quad \mathbb{P}(W|2)=6/10&\quad \mathbb{P}(W|3)=0\\
\mathbb{P}(G|1)=4/10&\quad \mathbb{P}(G|2)=1/10&\quad \mathbb{P}(G|3)=8/10\\
\end{alignat*}
The risk function is represented by,
\begin{align*}
R(\theta,\hat{\theta}_{p,q}) &= \mathbb{E}_{\theta}(|\theta^2-\hat{\theta}_{p,q}|^2)\\
&=\mathbb{E}_{\theta}\left(\sum_{i\in\{B,W,G\}}L(\theta,\hat{\theta}_{p,q}(i))\mathbb{P}(i|\theta)\right)
\end{align*}
where $L(\theta,\hat{\theta}_{p,q}(i))=L(\theta,1)p_i+L(\theta,2)q_i+L(\theta,3)(1-p_i-q_i)$.Therefore,
\begin{align*}
R(1,\hat{\theta}_{p,q}) &= [q_B+4(1-p_B-q_B)]\frac{4}{10}+[q_W+4(1-p_W-q_W)]\frac{2}{10}+[q_G+4(1-p_G-q_G)]\frac{4}{10}\\
R(2,\hat{\theta}_{p,q}) &= [9p_B+4q_B+49(1-p_B-q_B)]\frac{3}{10}+[9p_W+4q_W+49(1-p_W-q_W)]\frac{6}{10}\\
&+[9p_G+4q_G+49(1-p_G-q_G)]\frac{1}{10}
%+[9p_B+4q_B+49(1-p_B-q_B)]\frac{2}{10}
\end{align*}
\item Bayes risk is given by
\begin{align*}
r(f,\theta)=\int R(\theta,\hat{\theta}_{p,q})f(\theta)\,d\theta
\end{align*}
but since our scenario is discrete, we instead have
\begin{align*}
r(f,\theta) &= \sum_{\theta=1,2}R(\theta,\hat{\theta}_{p,q})\mathbb{P}(\theta)\\
&= \lambda R(1,\hat{\theta}_{p,q})+ (1-\lambda)R(2,\hat{\theta}_{p,q})
\end{align*}
where $R(1,\hat{\theta}_{p,q})$ and $R(2,\hat{\theta}_{p,q})$ are the values are from (a).
\item Given $\lambda = 1/2$, we have
\begin{align*}
r(f,\theta)=\frac{1}{2} \left(R(1,\hat{\theta}_{p,q})+ R(2,\hat{\theta}_{p,q})\right) &=\frac{1}{2}\left(16.3-13.6p_B-14.7q_B-24.8p_W-27.6q_W-5.6p_G-5.6q_G\right)
\end{align*}
thus to the infimum of Bayes risk is when $q_B = q_W = q_G =1$. This means the decision is to choose box 2 regardless of the color of the ball drawn.
\end{enumerate}

\newpage
\item


\begin{lstlisting}[language=R,commentstyle=\fontseries{lc}\color{gray}]
library(survey)
library(dplyr)

# read csv file into dataframe car
car <- read.csv('carmpgdat.csv')

# (a) Fitting a mulitple linear regression model

# generate a linear model with normally distributed noise with the model MPG~VOL+HP+SP+WT
# covariates <-cbind('VOL','HP','SP','WT')
lm_car <- lm(MPG~VOL+HP+SP+WT, data = car)


# estimated regression function and residual sum of squares
coefficients(lm_car)
# MPG = 192.43775332 - 0.01564501 * VOL + 0.39221231 * HP - 1.29481848 * SP - 1.85980373 * WT

RSS = sum(lm_car$residuals^2)
# RSS = 1027.381

# (b) Mallows's Cp
# Since the AIC criterion is equivalent to Mallows's Cp, 

# (i) Forward
base <- lm(MPG~WT, data = car)
step(base, scope = list(upper = lm_car, lower=~1), direction = 'forward', trace = TRUE)

# Start:  AIC=378.69
# MPG ~ 1
# 
# Df Sum of Sq    RSS    AIC
# + WT    1    6641.5 1466.0 240.45
# + HP    1    5058.0 3049.4 300.51
# + SP    1    3842.6 4264.9 328.02
# + VOL   1    1101.6 7005.9 368.72
# <none>              8107.5 378.69
# 
# Step:  AIC=240.45
# MPG ~ WT
# 
# Df Sum of Sq    RSS    AIC
# + SP    1    82.981 1383.0 237.68
# + HP    1    35.380 1430.6 240.45
# <none>              1466.0 240.45
# + VOL   1     3.883 1462.1 242.24
# 
# Step:  AIC=237.68
# MPG ~ WT + SP
# 
# Df Sum of Sq    RSS    AIC
# + HP    1    349.37 1033.7 215.80
# + VOL   1     45.97 1337.0 236.90
# <none>              1383.0 237.68
# 
# Step:  AIC=215.8
# MPG ~ WT + SP + HP
# 
# Df Sum of Sq    RSS   AIC
# <none>              1033.7 215.8
# + VOL   1    6.2685 1027.4 217.3
# 
# Call:
#   lm(formula = MPG ~ WT + SP + HP, data = car)
# 
# Coefficients:
#   (Intercept)           WT           SP           HP  
# 194.1296      -1.9221      -1.3200       0.4052 

# (ii) Backward
step(lm_car,direction = 'backward', trace = TRUE)

# Start:  AIC=217.3
# MPG ~ VOL + HP + SP + WT
# 
# Df Sum of Sq    RSS    AIC
# - VOL   1      6.27 1033.7 215.80
# <none>              1027.4 217.30
# - HP    1    309.67 1337.0 236.90
# - SP    1    373.36 1400.7 240.72
# - WT    1   1013.76 2041.2 271.59
# 
# Step:  AIC=215.8
# MPG ~ HP + SP + WT
# 
# Df Sum of Sq    RSS    AIC
# <none>              1033.7 215.80
# - HP    1    349.37 1383.0 237.68
# - SP    1    396.97 1430.6 240.45
# - WT    1   1322.87 2356.5 281.37
# 
# Call:
#   lm(formula = MPG ~ HP + SP + WT, data = car)
# 
# Coefficients:
#   (Intercept)           HP           SP           WT  
# 194.1296       0.4052      -1.3200      -1.9221  
# 
# Both forward and backward stepwise approach give the same submodel to be selected.
# The selected submodel is MPG ~ HP + SP + WT with the regression formula:
# MPG = 194.1296 + HP * 0.4052 - SP * 1.3200 - WT * 1.9221 


 
%# For the forward stepwise approach, the base model is important
%# as it will change the outcome. For example for this, if we were
%# to start with VOL instead of any of the other covariates, we 
%# will end up with also the VOL covariate. By not starting with with
%# the VOL covariate we will end up with a model without VOL which 
%# corresponds to the backward stepwise approach and also the 
%# Zheng-Loh model selection. As for the backward stepwise approach
%# we do not have such a problem as we start the model with all the 
%# covariates and reduce it down by computing the AIC of the model
%# with different covariate missing then remove the covariate that 
%# gives the smallest AIC when removed.



# (c) Zheng-Loh Model Selection
# Wald test for the covariates

regTermTest(lm_car, 'VOL', null=NULL,df=Inf, method = "Wald")
regTermTest(lm_car, 'HP', null=NULL,df=Inf, method = "Wald")
regTermTest(lm_car, 'SP', null=NULL,df=Inf, method = "Wald")
regTermTest(lm_car, 'WT', null=NULL,df=Inf, method = "Wald")

# Wald test for VOL
# in lm(formula = MPG ~ VOL + HP + SP + WT, data = car)
# Chisq =  0.4698075  on  1  df: p= 0.49308 
# 
# Wald test for HP
# in lm(formula = MPG ~ VOL + HP + SP + WT, data = car)
# Chisq =  23.20929  on  1  df: p= 1.4529e-06 
# 
# Wald test for SP
# in lm(formula = MPG ~ VOL + HP + SP + WT, data = car)
# Chisq =  27.98266  on  1  df: p= 1.2241e-07 
# 
# Wald test for WT
# in lm(formula = MPG ~ VOL + HP + SP + WT, data = car)
# Chisq =  75.97941  on  1  df: p= < 2.22e-16
# 
# Arranging in descending order we have:
#   WT > SP > HP > VOL
# We can do this as the Chisq test statistic is just the square 
# of the test statistic of the Wald test.
# 
# Let lm_j be the linear model with the jth largest Wald test statistic

n <- nrow(car)

lm_1 <- lm(MPG ~ WT, data = car)
jhat_1 = sum(lm_1$residuals^2) + (1 * RSS/(n-4) * log(n))
print (jhat_1)
# jhat_1 = 1524.045

lm_2 <- lm(MPG ~ WT + SP, data = car)
jhat_2 = sum(lm_2$residuals^2) + (2 * RSS/(n-4) * log(n))
print (jhat_2)
# jhat_2 = 1499.108

lm_3 <- lm(MPG ~ WT + SP + HP, data = car)
jhat_3 = sum(lm_3$residuals^2) + (3 * RSS/(n-4) * log(n)) 
print (jhat_3)
# jhat_3 =  1207.78

jhat_4 = RSS + (4 * RSS/(n-4) * log(n)) 
print (jhat_4)
# jhat_4 = 1259.555

# Thus the Zheng-Loh model selection method will select WT, SP and HP 
# as the covariates for predicting the MPG. Comparing it to (b), we see
# that the Zheng-Loh model selection method gives similar outcome to using
# Mallows's Cp model forward and backward stepwise to selecting a model.



\end{lstlisting}
\newpage

\item
\begin{lstlisting}[language=R,commentstyle=\fontseries{lc}\color{gray}]
library(dplyr)
library(magrittr)


# Reading data with separator tab
raw_riasec <- read.csv('RIASEC.csv',sep = '\t')

# (a)CLEANING UP
# List of realistic traits
realistic_trait <- c('R1','R2','R3','R4','R5','R6','R7','R8')

# Extracting out the realistic traits
raw_realistic <- raw_riasec[,realistic_trait]

# Removing rows with -1 from the dataframe
# realistic <- raw_realistic[raw_realistic != -1, ]

realistic <- raw_realistic %>%
  filter(R1 > 0 &  R2 > 0 & R3 > 0 & R4 > 0 & R5 > 0 & R6 > 0 & R7 > 0 & R8 > 0)
  

# (b)MODEL SELECTION

# Computing the score for the R trait
realistic <- realistic %>%
  rowwise() %>%
  mutate(Rscore = mean(c(R1, R2, R3, R4, R5, R6, R7, R8))) 

# Generating the training and validation set 
tr_realistic <- realistic[1:6500,]
val_realistic <- realistic[-(1:6500),]

# Building the linear model
lm_riasec <- lm(Rscore~R1, data = tr_realistic)
RSS_tr = sum(lm_riasec$residuals^2)
print(RSS_tr)
# RSS_tr = 2902.039

# estimated regression function and residual sum of squares
print(lm_riasec$coefficients)
# R1 = 1.0181052 + 0.4235993 * R1


# (c)VALIDATION
reg.fn <- function(x) 1.0181052 + 0.4235993 * x

val_realistic %<>%
  mutate(pred_Rscore = reg.fn(R1),
         residuals = reg.fn(R1) - Rscore )

avg_RSS_tr = mean(lm_riasec$residuals^2)
avg_RSS_val = mean(val_realistic$residuals^2)

print (avg_RSS_tr) # 0.4464676
print (avg_RSS_val) # 0.5201091

# The residual sum of squares for the validation set using the regression function
# is larger than the residual sum of square for the training set but are of the
# same order. Thus the model generalizes well.





\end{lstlisting}
\newpage

\item
\begin{enumerate}[(a)]
\item 
%\begin{align*}
%I = \int_{1}^{2}f(x|1.5,2.3) \,dx= \frac{1}{N}\sum_{i=1}^{N}f(X_i|1.5,2.3)
%\end{align*}

\begin{lstlisting}[language=R,commentstyle=\fontseries{lc}\color{gray}]
# (a) Using Monte Carlo method with N samples

N <- 100000
I <- function(u){X = rgamma(1.5,2.3,n = N); return(sum(1<X & X<2)/N)}
I_hat <- I

# I_hat = 0.17552

# Estimated standard error by resampling Monte Carlo 10000 times.
se_mc <- sqrt(var(sapply(1:N,I)))

# se_mc =   0.001204692

# Estimated standard error using 10000 bootstrap samples.
B <- 10000
base_sample <- rgamma(1.5,1/2.3,n = N)
I_bootstrap <- function(u){X = sample(base_sample, N, replace = TRUE);return(sum(1<X & X<2)/N)}
se_bootstrap <- sqrt(var(sapply(1:B,I_bootstrap)))

# se_bootstrap = 0.001286245



\end{lstlisting}
%\begin{python}
%def mc_integrate(alpha, beta, N = 100000):
%    sample = np.random.uniform(1,2, size = (1, N))
%    return np.mean([gamma.pdf(x, alpha, loc = 0, scale = beta) for x in sample])   
%mc_integrate(1.5, 2.3)    
%\end{python}
%Estimate $I$ using Monte Carlo method is 0.20449041416849226.
%\begin{python}
%# Empirical distribution to draw bootstrap samples
%N = 100000
%sample = np.random.uniform(1,2, size = (1, N))
%emp_dist = [gamma.pdf(x, 1.5, loc = 0, scale = 2.3) for x in sample]
%# Creating the 10000 bootstrap samples
%bs_samples = [np.random.choice(emp_dist[0], 100000) for x in range(10000)]
%# Getting an estimate of I for each bootstrap sample
%bs_estimates = [np.mean(bs_samples[i]) for i in range(10000)]
%# Computing the standard error obtained using bootstrap method
%mean_bs_estimates = np.mean(bs_estimates)
%bs_se = np.mean([(mean_bs_estimates - bs_estimates[i])**2 for i in range(10000)])
%
%# Using the Monte Carlo method
%# Resampling the distribution 10000 times 
%mc_estimates = [mc_integrate(1.5, 2.3) for i in range(10000)]
%# Computing the standard error obtained from MC method
%mean_mc_estimates = np.mean(mc_estimates)
%mc_se = np.mean([(mean_mc_estimates - mc_estimates[i])**2 for i in range(10000)])
%\end{python}
%Bootstrap method gives a standard error of 3.23599044314e-10 and MC method gives a standard error of 3.32551091852e-10.
\item 
We see that we can rewrite $I$ as the following,
\begin{align*}
I = \int_{1}^{2}f(x|1.5,2.3)\,dx = \int_{0}^{\infty}h(x)f(x|1.5,2.3)\,dx
\end{align*}
where $h(x) = \mathbbm{1}(1\leq x \leq 2)$, an indicator function that is 1 when $1 \leq x \leq 2$ and 0 otherwise. Thus we can use the following estimator $\hat{I}$ for $I$ where $X_i\sim \text{Gamma}(1.5,2.3)$,
\begin{align*}
\hat{I} = \frac{1}{N}\sum_{i=1}^{N}h(X_i)
\end{align*}
Hence by viewing $h(X)\sim\text{Bernoulli}(p)$, where $p = \mathbb{P}(1\leq X \leq 2)$ for $X\sim \text{Gamma}(1.5,2.3)$ the standard error is given by 
\begin{align*}
Var(\hat{I}) = \frac{p(1-p)}{N}
\end{align*}
where we sourced for the value of $p$ from the website thus getting
\begin{align*}
\mathbb{P}(1\leq X \leq 2)=\int_{0}^{2}f(x|1.5,2.3)\,dx-\int_{0}^{1}f(x|1.5,2.3)\,dx=0.97325-0.79645%0.37173-0.16723
\end{align*}
Evaluating the value of the standard error we get $\text{\sffamily{se}} =$ 0.0012064069
%
%The standard error of $\hat{I}$ which is the estimated $I$ value using Monte Carlo method where $X_i \sim U(1,2)$ is given by
%\begin{align*}
%Var(\hat{I})&=Var\left(\frac{1}{N}\sum_{i=1}^{N}f(X_i|1.5,2.3)\right)\\
%&=\frac{1}{N}Var\left(f(X|1.5,2.3)\right), ~~\text{since the $X_i$'s are iid}
%\end{align*}
%Thus we need the first and second moments of $Y= f(X|1.5,2.3)$ where $X \sim U(1,2)$
%\begin{align*}
%\mathbb{E}\left(Y\right) & = \int_{1}^{2}f(x|1.5,2.3)\,dx = \int_{0}^{2}f(x|1.5,2.3)\,dx-\int_{0}^{1}f(x|1.5,2.3)\,dx=0.37173-0.16723\\
%\mathbb{E}\left(Y^2\right) & = \int_{1}^{2}f(x|1.5,2.3)^2\,dx
%\end{align*}
\end{enumerate}
\end{enumerate}

\end{document}