\begin{flushright}
Date: 031116
\end{flushright}



\begin{thm}
There exists simple no-regret algorithms such that the expected regret of every action is 
\begin{align*}
\mathcal{O}\left(\sqrt{\frac{\ln n}{T}}\right)
\end{align*}
\end{thm}
An easy consequence of the theorem above is presented in the following collary.
\begin{cor}
There exists an online learning algorithm such that for every $\epsilon >0$, has expected regret of at most $\epsilon$ after $\mathcal{O}\left(\ln n/\epsilon^2\right)$
\end{cor}

\subsection{Algorithm}
\begin{enumerate}
\item Initialize $w^1(a)=1$ for every $a \in A$.
\item For $t=1,2,\ldots, T$
\begin{enumerate}[(a)]
\item Play an action according to the distribution $p^t:=w^t/\Gamma^t$, where $\Gamma^t=\sum_{a\in A}w^t(a)$ is the sum of the weights.
\item Given the cost vector $c^t$, decrease the weights using the formula 
\begin{align*}
w^{t+1}(a)=w^t(a)\cdot (1-\epsilon)^{c^t(a)}
\end{align*}
for every action $a \in A$.


\end{enumerate}
\end{enumerate}

The construction of the algorithm is such that when the cost is 0, then the weight remains the same and when the cost is 1, the weight gets reduced by a factor of $1-\epsilon$. Considering what happens to the distribution of the weights when we vary $\epsilon$, for small values of $\epsilon$, the weights are slowly eroded and the distribution $p^t$ tends to the uniform distribution (needs a more rigor argument as to why).When $\epsilon$ is close to 1, we see that the weight is concentrated on the action that has accumulated the least cost so far. This can be observed by the analysis of the algorithm below. For all $a \in A$, we start with $w^1(a)=1$
\begin{align*}
w^{t+1}(a)&= f(w^t(a),c^t(a))\\
&= w^t(a)\cdot(1-\epsilon)^{c^t(a)}\\
&=\quad\vdots\\
&= w^1(a)\cdot (1-\epsilon)^{\sum_{k=1}^{t-1}c^k(a)}
\end{align*}


Now we would like to connect the expected performance at day $t$ denoted by $V^t$, the optimal performance (OPT) and $w^t(a)$ together. Let $a^\ast = \max_{a \in A}w^T(a)$, then
\begin{align}
\Gamma^T=\sum_{a \in A} w^T(a) &\geq w^T(a^\ast)\notag\\
&= w^1(a^\ast)\cdot (1-\epsilon)^{\sum_{k=1}^{t-1}c^k(a^\ast)}\notag\\
&=  (1-\epsilon)^{\text{OPT}}\label{eq:earlierresult}
\end{align}
the expected performance at time $t$ to be
\begin{align*}
V^t=\sum_{a\in A}p^t(a)\cdot c^t(a)=\sum_{a \in A}\frac{w^t(a)}{\Gamma^t}\cdot c^t(a)
\end{align*}
with the prelimaries done, we are ready to connect them together. We now try to find a recursive equation relating $\Gamma^{t+1}$ and $\Gamma^t$:
\begin{align*}
\Gamma^{t+1}&=\sum_{a\ in A} w^{t+1}(a)\\
&=\sum_{a\ in A} w^{t}(a)(1-\epsilon)^{c^t(a)}\\
&\leq \sum_{a\in A} w^{t}(a)(1-\epsilon\cdot c^t(a)), \quad \text{by lemma \ref{lemma:epsilon}}\\
&= \Gamma^t\sum_{a\in A} \frac{w^{t}(a)}{\Gamma^t}(1-\epsilon\cdot c^t(a))\\
&= \Gamma^t\sum_{a\in A} p^{t}(a)(1-\epsilon\cdot c^t(a))\\
&= \Gamma^t\sum_{a\in A} p^{t}(a)-\epsilon\sum_{a\in A}p^t(a) c^t(a)\\
&= \Gamma^t\left(1-\epsilon V^t\right)\\
\implies& \Gamma^{t+1} \leq \Gamma^1 \prod_{i=1}^{t}(1-\epsilon\cdot V^i)\\
\text{By (\ref{eq:earlierresult})	,}~~(1-\epsilon)^{\text{OPT}}&\leq\Gamma^{t+1} \leq \Gamma^1 \prod_{i=1}^{t}(1-\epsilon\cdot V^i)
\end{align*}
Taking $\ln$,
\begin{align*}
\text{OPT}\cdot \ln(1-\epsilon)&\leq \ln n + \sum_{i=1}^{t} \ln (1-\epsilon \cdot V^i)\\
\text{OPT}\cdot (-\epsilon-\epsilon^2)&\leq \ln n - \epsilon\sum_{i=1}^{t}  V^i, \quad \text{by Lemma \ref{lemma:ln}}\\
\sum_{i=1}^{t}  V^i &\leq \frac{\ln n}{\epsilon}+(1+\epsilon)\text{OPT}\\
&\leq \text{OPT}+\frac{\ln n}{\epsilon}+\epsilon\text{OPT}
\end{align*}
Here we realize that in the last eqution decreasing $\epsilon$ increases $\ln n /\epsilon$ and increasing $\epsilon$ increases $\epsilon \text{OPT}$. Thus to keep the upper bound low, we keep them equal, which then we obtain $\epsilon = \sqrt{\ln n /T}$. Hence the cumulative expected cost of the no regret algorithm is at most $2\sqrt{T\ln n}$ more than the cumulative cost of the optimal.

\begin{lemma}
For $\epsilon \in [0,1]$ and $x \in [0,1]$, 
\begin{align*}
(1-\epsilon)^x \leq (1-\epsilon x)
\end{align*}\label{lemma:epsilon}
\end{lemma}

\begin{lemma}
If $x \in [0,1/2]$
\begin{align*}
-x-x^2 \leq \ln (1-x) \leq -x
\end{align*}\label{lemma:ln}
\end{lemma}
Next we show that in a game, i.e. routing game where everyone is using a no regret algorithm, the state of the game converges to a coarse correlated equilibrium. In each time step $t=1,2,\ldots, T$ of no regret dynamics:
\begin{enumerate}
\item Each player $i$ chooses simultaneously and independently a mixed strategy $p^t_i$ using a no regret algorithm of their choice.
\item Each player receives a cost vector $c^t_i$ where $c_i^t(s_i)$ is the expected cost of strategy $s_i$ when the other players play their chosen mixed strategies, i.e. $c^t_i(s_i)=\mathbf{E}_{s_{-i}\sim \sigma_{-i}}[c_i(s_i,s_{-i})]$ where $\sigma_{-i}=\prod_{j\neq i}\sigma_j$.
\end{enumerate}


\begin{thm}
Suppose after $T$ iterations of no-regret dynamics, every player of a cost minimization game has a regret of at most $\epsilon$ for each of its strategies. Let $\sigma=\prod_{i=1}^{k}p^t_i$ denote the outcome distribution at time $t$ and $\sigma=\frac{1}{T}\sum_{t=1}^{T}\sigma^t$ the time average history of these distributions. Then $\sigma$ is an $\epsilon-$approximate coarse correlated equilibrium, in the sense that 
\begin{align*}
\mathbf{E}[c_i(\mathbf{s})]\leq\mathbf{E}[c_i(s'_i,s_{-i})]+\epsilon
\end{align*}
for every player $i$ and unilateral decision $s'_i$.
\end{thm}


\begin{cor}
Suppose after $T$ iterations of no regret dynamics, player $i$ has expected regret at most $R_i$ for each of its actions. Then the time average expected objective function value $\frac{1}{T}\mathbf{E}_{\mathbf{s}\sim\sigma_i}$ is at most 
\begin{align*}
\frac{\lambda}{1-\mu} cost(s^\ast)+\frac{\sum_{i=1}^{k}R_i}{1-\mu}
\end{align*}
In particular, as $T \to \infty, \sum_{i=1}^{k}R_i\to 0$ and the guarantee to converge to the standard PoA bound $\frac{\lambda}{1-\mu}$.
\end{cor}
%\section{Best Response Dynamics}
%While the current outcome is not a Pure Nash equilibirum (PNE), we can pick an arbitrary player $i$ and an arbitrary beneficial deviation $s'_i$ for player $i$ and move to outcome $(s'_i,\mathbf{s_{-i}})$.
%
%Recall that the definition of a potential game is one where there exists a function $\Phi:\mathcal{S}\to \mathbb{R}$ where $\mathcal{S}$ is the finite set of strategies with
%\begin{align*}
%\Phi(s'_i,s_{-i})-\Phi(s_i,s_{-i})=c_i(s'_i,s_{-i})-c_i(s_i,s_{-i})
%\end{align*}
%
%\begin{prop}
%In a finite potential game from any arbitrary outcome, best-response dynamics converge to a PNE.
%\begin{proof}
%In a best-response dynamics approach, every iteration has $\Phi(\mathbf{s^{t+1}})<\Phi(\mathbf{s^t})$, i.e. the potential decreases. Unless the $\mathbf{s^t}$ is a PNE, our $\Phi$ is lower bounded by $\min_{s\in \mathcal{S}}\Phi(s)$ and hence the process must terminate.
%\end{proof}
%\end{prop}
%
%\begin{defn}[$\epsilon-$Pure Nash Equilibrium]
%For $\epsilon \in [0,1]$, and outcome $\mathbf{s}$ is an $\epsilon-$pure NE if for every agent $i$ and deviations $s'_i\in S_i$
%\begin{align*}
%c_i(s'_i,s_{-i})\geq (1-\epsilon)c_i(s_i,s_{-i})
%\end{align*}
%\end{defn}
%
%An $\epsilon-$best response dynamics is one which permits moves when there is significant improvements (substential lowering of cost or increasing of utility) which is an important factor to for a state to converge to near optimal equilibrium. While a current outcome $\mathbf{s}$ is not an $\epsilon-$PNE, we pick an arbitary player $i$ that has an $\epsilon-$move, i.e. a deviation to $s'_i$:
%\begin{align*}
%c_i(s'_i,s_{-i})<(1-\epsilon)c_i(\mathbf{s})
%\end{align*}
%
%
%\begin{lemma}
%For $x\in (0,1)$
%\begin{align*}
%(1-x)^{1/x}\leq (e^{-x})^{1/x}=e^{-1}
%\end{align*}
%\label{lemma:exp}
%\end{lemma}
%
%
%\begin{thm}[Fast convergence of $\epsilon-$Best Response Dynamics]
%Consider an atomic selfish routing game where:
%\begin{enumerate}[1.]
%\item All players have the same source $s$ and destination $t$ vertex.
%\item Cost function satisfy the ``$\alpha-$bound jump condition''
%\begin{align*}
%c_e(x)\leq c_e(x+1)\leq\alpha\cdot c_e(x)
%\end{align*}
%for all edges $e$.
%\item The MaxGain variant of $\epsilon-$BR dynamics is used: in every iteration, amongst all players with an $\epsilon-$move available, the player who can obtain the biggest absolute cost decrease gets to move.
%\end{enumerate}
% Then an $\epsilon-$PNE is reached in at most
% \begin{align*}
% \frac{k \cdot \alpha}{\epsilon}\log\frac{\Phi(\mathbf{s^0})}{\Phi_{min}}
% \end{align*}
% iterations, where $k$ is the number of agents, $\mathbf{s^0}$ is the initial state of the system.
% \begin{proof}
% Using lemma \ref{lemma:highcostagent} we pick the agent $i$ with the highest cost to get
% \begin{align*}
% \Phi(\mathbf{s})-\Phi(s'_i,s_{-i})= c_i(\mathbf{s})-c_i(s'_i,s_{-i}) &\geq \frac{\epsilon}{\alpha k}\cdot c_i(\mathbf{s}),\quad \text{by Lemma \ref{lemma:costofdevplayer}}\\
% &\geq \frac{\epsilon}{\alpha 	k}\cdot \Phi(\mathbf{s})
% \end{align*}
% thus we have
% \begin{align*}
%\left(1-\frac{\epsilon}{\alpha 	k}\right) \Phi(\mathbf{s^t}) \geq \Phi(\mathbf{s^{t+1}}) 
% \end{align*}
% thus using Lemma \ref{lemma:exp} we obtain that an $\epsilon-$PNE is reached in $\frac{\alpha k}{\epsilon}\log \frac{\Phi(\mathbf{s^0})}{\Phi_{min}}$ iterations.
% \end{proof}
%\end{thm}
%
%The two lemmas below are the ones used in the proof.
%
%\begin{lemma}
%For all $\mathbf{s}\in\mathcal{S}$ there exists an agent such that
%\begin{align*}
%c_i(s)\geq \frac{\Phi(\mathbf{s})}{k}
%\end{align*}
%\begin{proof}
%Recall that $\Phi(\mathbf{s})\leq cost(\mathbf{s})$, then pick the agent that realizes the highest cost, $i=\argmax_{i}c_i(\mathbf{s})$, then
%\begin{align*}
%c_i(\mathbf{s})\geq \frac{cost(\mathbf{s})}{k}\geq \frac{\Phi(\mathbf{s})}{k}
%\end{align*}
%
%\end{proof}
%\label{lemma:highcostagent}
%\end{lemma}
%
%\begin{lemma}
%Suppose player $i$ is chosen at outcome $s$ by MaxGain $\epsilon-$best response dynamics and he takes the $\epsilon-$ move $s'_i$,then
%\begin{align}
%c_i(\mathbf{s})-c_i(s'_i,s_{-i})\geq \frac{\epsilon}{\alpha}c_j(\mathbf{s})\label{eq:costofdevplayer}
%\end{align}
%for any other agent $j$.
%\label{lemma:costofdevplayer}
%\begin{proof}
%For the case when $j=i$, when $\alpha=1$, it is exactly the definition of the $\epsilon-$move. Now consider when $i \neq j$ with $j$ having an $\epsilon-$move, by MaxGain dynamics and $\alpha=1$,
%\begin{align*}
%c_i(\mathbf{s})-c_i(s'_i,s_{-i})\geq c_j(\mathbf{s})-c_j(s'_j,s_{-j})>\epsilon\cdot c_j(\mathbf{s})
%\end{align*}
%the proof is completed by with the case where $j$ does not have an $\epsilon-$move, which we consider $-$ since $s'_i$ is such a great deviation for player $i$, why isn't it good for player $j$? That is 
%\begin{align*}
%c_i(s'_i,s_{-i})<(1-\epsilon)c_i(\mathbf{s})
%\end{align*}
%while
%\begin{align*}
%c_j(s'_i,s_{-j})\geq(1-\epsilon)c_j(\mathbf{s})
%\end{align*}
%
%and here we used the condition that the agents have the same source and sink vertex, i.e. they have the same set of strategies. An observation made here is that $(s'_i,s_{-i})$ and $(s'_i,s_{-j})$ have at least $k-1$ strategies in common (note that $s'_i$ is played by agent $i$ in the former and agent $j$ in the latter and the $k-2$ players other than $i$ and $j$ are playing fixed strategies.) Thus by the ``$\alpha-$bound jump condition'', we have
%\begin{align*}
%c_j(s'_i,s_{-j})\leq \alpha c_i(s'_i,s_{-i})
%\end{align*}
%using the inequality above
%\begin{align*}
%c_i(\mathbf{s})-c_i(s'_i,s_{-i})>\epsilon c_i(\mathbf{s})\geq \frac{\epsilon}{\alpha}c_j(\mathbf{s})
%\end{align*}
%which completes the proof.
%\end{proof}
%\end{lemma}
%
%
%
%
%\begin{lemma}
%For $x\in (0,1)$
%\begin{align*}
%(1-x)^{1/x}\leq (e^{-x})^{1/x}=e^{-1}
%\end{align*}
%\label{lemma:exp}
%\end{lemma}
%
%
%\begin{thm}
%Consider a $(\lambda,\mu)-$cost minimization game with a positive potential function $\Phi$ such that $\Phi(\mathbf{s}) \leq cost(\mathbf{s})$ for every outcome $\mathbf{s}$. Let $\mathbf{s^0},\mathbf{s^1},\ldots,\mathbf{s^T}$ be a sequence generated by MaxGain best response dynamics, $\mathbf{s^\ast}$ a minimum cost outcome and $1>\gamma>0$ is a parameter, Then for all but 
%\begin{align}
%O\left(
%\frac{k}{\gamma(1-\mu)}\log\frac{\Phi(\mathbf{s^0})}{\Phi_{min}}\label{eq:outcomes}
%\right)
%\end{align}
%outcomes $\mathbf{s}^t$ satisfy
%\begin{align*}
%cost(\mathbf{s^t})\leq \left(\frac{\lambda}{1-\mu}+\frac{1}{1-\gamma}\right)\cdot cost(\mathbf{s^\ast})
%\end{align*}
%\begin{align}
%cost(\mathbf{s^t})\leq \left(\frac{\lambda}{1-\mu}+\gamma\right)\cdot cost(\mathbf{s^\ast})
%cost(\mathbf{s^t})\leq \left(\frac{\lambda}{(1-\mu)(1-\gamma)}\right)\cdot cost(\mathbf{s^\ast})\, \label{eq:good}
%\end{align}
%\begin{proof}
%\begin{align}
%cost(\mathbf{s^t})&\leq \sum_i c_i(\mathbf{s^t})\notag\\
%&=\sum_i\left[c_i(s_i^\ast,s_{-i}^t)+\delta_i(\mathbf{s^t})\right],\quad \delta_i(\mathbf{s^t})=c_i(\mathbf{s^t})-c_i(s_i^\ast,s^t_{-i})\notag\\
%&\leq \lambda \cdot cost(\mathbf{s^\ast})+\mu \cdot cost(\mathbf{s^t})+\sum_i\delta_i(\mathbf{s^t})\notag\\
%cost(\mathbf{s^t})&\leq \frac{\lambda}{1-\mu}\cdot cost(\mathbf{s^\ast}) + \frac{1}{1-\mu}\cdot \sum_i\delta_i(\mathbf{s^t}) \label{eq:2}
%\end{align}
%we shall let $\Delta(\mathbf{s^t})=\sum_i\delta_i(\mathbf{s^t})$ in the remaining parts of the proof. We shall now define a state $\mathbf{s^t}$ to be bad if it does not satisfy (\ref{eq:good}) and 
%\begin{align}
%cost(\mathbf{s^t})>\left(\frac{\lambda}{1-\mu}+\frac{1}{1-\gamma}\right)\cdot cost(\mathbf{s^t})
%\end{align} 
%\begin{align*}
%cost(\mathbf{s^t})> \left(\frac{\lambda}{1-\mu}+\gamma\right)\cdot cost(\mathbf{s^\ast})
%\end{align*}
%by (\ref{eq:2}), when $\mathbf{s^t}$ is bad we get
%\begin{align*}
%\Delta(\mathbf{s^t})&\geq \gamma(1-\mu)\cdot cost(\mathbf{s^t})\\
%\end{align*}
%By the MaxGain definition and the inequality relating the potential function and cost,
%\begin{align*}
%\max_{i}\delta_i(\mathbf{s^t})\geq \frac{\Delta(\mathbf{s^t})}{k}\geq \frac{\gamma(1-\mu)}{k}\cdot cost(\mathbf{s^t})\geq \frac{\gamma(1-\mu)}{k}\cdot \Phi(\mathbf{s^t})
%\end{align*}
%and we get what we desire as
%\begin{align*}
%\Phi(\mathbf{s^t})-\Phi(s_i^\ast,s^t_{-i})
%=c_i(\mathbf{s^t})-c_i(s_i^\ast,s^t_{-i})&=\delta_i(\mathbf{s^t})
%\end{align*}
%and hence
%\begin{align}
%\left(1-\frac{\gamma(1-\mu)}{k}\right)\Phi(\mathbf{s^t})\geq \Phi(s_i^\ast,s^t_{-i})
%\left(1-\frac{\gamma(1-\mu)}{k}\right)\Phi(\mathbf{s^t})\geq \Phi(\mathbf{s^{t+1}})\label{eq:result}
%\end{align}
%whenever $\mathbf{s^t}$ is a bad state. The equation in (\ref{eq:result}) says that for every MaxGain best response dynamics, if the state is bad, the new state $\mathbf{s^{t+1}}$ is smaller than the previous state $\mathbf{s^t}$ by a factor of $1-\frac{\gamma(1-\mu)}{k}$. By Lemma \ref{lemma:exp}, the potential decreases by a factor of $e$ for every $\frac{k}{\gamma(1-\mu)}$ bad states encountered. Thus solving 
%\begin{align*}
%e^{-n}\Phi(\mathbf{s^0}) \geq \Phi_{min}
%\end{align*}
%shows (\ref{eq:outcomes}).
%\end{proof}
%\end{thm}
