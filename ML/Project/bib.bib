Automatically generated by Mendeley Desktop 1.17.13
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Bottou2011,
abstract = {A plausible definition of "reasoning" could be "algebraically manipulating previously acquired knowledge in order to answer a new question". This definition covers first-order logical inference or probabilistic inference. It also includes much simpler manipulations commonly used to build large learning systems. For instance, we can build an optical character recognition system by first training a character segmenter, an isolated character recognizer, and a language model, using appropriate labeled training sets. Adequately concatenating these modules and fine tuning the resulting system can be viewed as an algebraic operation in a space of models. The resulting model answers a new question, that is, converting the image of a text page into a computer readable text. This observation suggests a conceptual continuity between algebraically rich inference systems, such as logical or probabilistic inference, and simple manipulations, such as the mere concatenation of trainable learning systems. Therefore, instead of trying to bridge the gap between machine learning systems and sophisticated "all-purpose" inference mechanisms, we can instead algebraically enrich the set of manipulations applicable to training systems, and build reasoning capabilities from the ground up.},
archivePrefix = {arXiv},
arxivId = {1102.1808},
author = {Bottou, Leon},
doi = {10.1007/s10994-013-5335-x},
eprint = {1102.1808},
file = {:C$\backslash$:/Users/zlai/Desktop/PDFs/papers/1102.1808.pdf:pdf},
isbn = {0885-6125},
issn = {0885-6125},
journal = {Arxiv preprint arXiv11021808},
pages = {15},
title = {{From Machine Learning to Machine Reasoning}},
url = {http://arxiv.org/abs/1102.1808},
year = {2011}
}

@article{Santoro2017,
abstract = {Relational reasoning is a central component of generally intelligent behavior, but has proven difficult for neural networks to learn. In this paper we describe how to use Relation Networks (RNs) as a simple plug-and-play module to solve problems that fundamentally hinge on relational reasoning. We tested RN-augmented networks on three tasks: visual question answering using a challenging dataset called CLEVR, on which we achieve state-of-the-art, super-human performance; text-based question answering using the bAbI suite of tasks; and complex reasoning about dynamic physical systems. Then, using a curated dataset called Sort-of-CLEVR we show that powerful convolutional networks do not have a general capacity to solve relational questions, but can gain this capacity when augmented with RNs. Our work shows how a deep learning architecture equipped with an RN module can implicitly discover and learn to reason about entities and their relations.},
archivePrefix = {arXiv},
arxivId = {1706.01427},
author = {Santoro, Adam and Raposo, David and Barrett, David G. T. and Malinowski, Mateusz and Pascanu, Razvan and Battaglia, Peter and Lillicrap, Timothy},
doi = {10.1109/WACV.2017.108},
eprint = {1706.01427},
file = {:C$\backslash$:/Users/zlai/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Santoro et al. - 2017 - A simple neural network module for relational reasoning.pdf:pdf},
isbn = {9781509048229},
issn = {21607516},
pages = {1--16},
title = {{A simple neural network module for relational reasoning}},
url = {http://arxiv.org/abs/1706.01427},
year = {2017}
}

@article{Mascharka2018,
abstract = {Visual question answering requires high-order reasoning about an image, which is a fundamental capability needed by machine systems to follow complex directives. Recently, modular networks have been shown to be an effective framework for performing visual reasoning tasks. While modular networks were initially designed with a degree of model transparency, their performance on complex visual reasoning benchmarks was lacking. Current state-of-the-art approaches do not provide an effective mechanism for understanding the reasoning process. In this paper, we close the performance gap between interpretable models and state-of-the-art visual reasoning methods. We propose a set of visual-reasoning primitives which, when composed, manifest as a model capable of performing complex reasoning tasks in an explicitly-interpretable manner. The fidelity and interpretability of the primitives' outputs enable an unparalleled ability to diagnose the strengths and weaknesses of the resulting model. Critically, we show that these primitives are highly performant, achieving state-of-the-art accuracy of 99.1{\%} on the CLEVR dataset. We also show that our model is able to effectively learn generalized representations when provided a small amount of data containing novel object attributes. Using the CoGenT generalization task, we show more than a 20 percentage point improvement over the current state of the art.},
archivePrefix = {arXiv},
arxivId = {1803.05268},
author = {Mascharka, David and Tran, Philip and Soklaski, Ryan and Majumdar, Arjun},
doi = {10.1109/CVPR.2018.00519},
eprint = {1803.05268},
file = {:C$\backslash$:/Users/zlai/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mascharka et al. - 2018 - Transparency by Design Closing the Gap Between Performance and Interpretability in Visual Reasoning.pdf:pdf},
title = {{Transparency by Design: Closing the Gap Between Performance and Interpretability in Visual Reasoning}},
url = {http://arxiv.org/abs/1803.05268},
year = {2018}
}

@article{Weston2015,
abstract = {One long-term goal of machine learning research is to produce methods that are applicable to reasoning and natural language, in particular building an intelligent dialogue agent. To measure progress towards that goal, we argue for the usefulness of a set of proxy tasks that evaluate reading comprehension via question answering. Our tasks measure understanding in several ways: whether a system is able to answer questions via chaining facts, simple induction, deduction and many more. The tasks are designed to be prerequisites for any system that aims to be capable of conversing with a human. We believe many existing learning systems can currently not solve them, and hence our aim is to classify these tasks into skill sets, so that researchers can identify (and then rectify) the failings of their systems. We also extend and improve the recently introduced Memory Networks model, and show it is able to solve some, but not all, of the tasks.},
archivePrefix = {arXiv},
arxivId = {1502.05698},
author = {Weston, Jason and Bordes, Antoine and Chopra, Sumit and Rush, Alexander M. and van Merri{\"{e}}nboer, Bart and Joulin, Armand and Mikolov, Tomas},
doi = {10.1016/j.jpowsour.2014.09.131},
eprint = {1502.05698},
file = {:C$\backslash$:/Users/zlai/Desktop/PDFs/papers/1502.05698.pdf:pdf},
isbn = {1502.05698},
issn = {03787753},
title = {{Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks}},
url = {http://arxiv.org/abs/1502.05698},
year = {2015}
}

@article{Cer2018,
abstract = {We present models for encoding sentences into embedding vectors that specifically target transfer learning to other NLP tasks. The models are efficient and result in accurate performance on diverse transfer tasks. Two variants of the encoding models allow for trade-offs between accuracy and compute resources. For both variants, we investigate and report the relationship between model complexity, resource consumption, the availability of transfer task training data, and task performance. Comparisons are made with baselines that use word level transfer learning via pretrained word embeddings as well as baselines do not use any transfer learning. We find that transfer learning using sentence embeddings tends to outperform word level transfer. With transfer learning via sentence embeddings, we observe surprisingly good performance with minimal amounts of supervised training data for a transfer task. We obtain encouraging results on Word Embedding Association Tests (WEAT) targeted at detecting model bias. Our pre-trained sentence encoding models are made freely available for download and on TF Hub.},
archivePrefix = {arXiv},
arxivId = {1803.11175},
author = {Cer, Daniel and Yang, Yinfei and Kong, Sheng-yi and Hua, Nan and Limtiaco, Nicole and John, Rhomni St. and Constant, Noah and Guajardo-Cespedes, Mario and Yuan, Steve and Tar, Chris and Sung, Yun-Hsuan and Strope, Brian and Kurzweil, Ray},
doi = {10.1007/s11423-014-9358-1},
eprint = {1803.11175},
file = {:C$\backslash$:/Users/zlai/Desktop/PDFs/papers/1803.11175.pdf:pdf},
issn = {1042-1629},
title = {{Universal Sentence Encoder}},
url = {http://arxiv.org/abs/1803.11175},
year = {2018}
}