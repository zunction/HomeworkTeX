Automatically generated by Mendeley Desktop 1.17.13
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop
@incollection{NIPS2017_7181,
title = {Attention is All you Need},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {5998--6008},
year = {2017},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf}
}
@inproceedings{snli:emnlp2015,
	Author = {Bowman, Samuel R. and Angeli, Gabor and Potts, Christopher and Manning, Christopher D.},
	Booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	Publisher = {Association for Computational Linguistics},
	Title = {A large annotated corpus for learning natural language inference},
	Year = {2015}
}
@article{RajpurkarZLL16,
  author    = {Pranav Rajpurkar and
               Jian Zhang and
               Konstantin Lopyrev and
               Percy Liang},
  title     = {SQuAD: 100, 000+ Questions for Machine Comprehension of Text},
  journal   = {CoRR},
  volume    = {abs/1606.05250},
  year      = {2016},
  url       = {http://arxiv.org/abs/1606.05250},
  archivePrefix = {arXiv},
  eprint    = {1606.05250},
  timestamp = {Mon, 13 Aug 2018 16:49:13 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/RajpurkarZLL16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{Bottou2011,
abstract = {A plausible definition of "reasoning" could be "algebraically manipulating previously acquired knowledge in order to answer a new question". This definition covers first-order logical inference or probabilistic inference. It also includes much simpler manipulations commonly used to build large learning systems. For instance, we can build an optical character recognition system by first training a character segmenter, an isolated character recognizer, and a language model, using appropriate labeled training sets. Adequately concatenating these modules and fine tuning the resulting system can be viewed as an algebraic operation in a space of models. The resulting model answers a new question, that is, converting the image of a text page into a computer readable text. This observation suggests a conceptual continuity between algebraically rich inference systems, such as logical or probabilistic inference, and simple manipulations, such as the mere concatenation of trainable learning systems. Therefore, instead of trying to bridge the gap between machine learning systems and sophisticated "all-purpose" inference mechanisms, we can instead algebraically enrich the set of manipulations applicable to training systems, and build reasoning capabilities from the ground up.},
archivePrefix = {arXiv},
arxivId = {1102.1808},
author = {Bottou, Leon},
doi = {10.1007/s10994-013-5335-x},
eprint = {1102.1808},
file = {:C$\backslash$:/Users/zlai/Desktop/PDFs/papers/1102.1808.pdf:pdf},
isbn = {0885-6125},
issn = {0885-6125},
journal = {Arxiv preprint arXiv11021808},
pages = {15},
title = {{From Machine Learning to Machine Reasoning}},
url = {http://arxiv.org/abs/1102.1808},
year = {2011}
}
@InProceedings{N18-1202,
  author = 	"Peters, Matthew
		and Neumann, Mark
		and Iyyer, Mohit
		and Gardner, Matt
		and Clark, Christopher
		and Lee, Kenton
		and Zettlemoyer, Luke",
  title = 	"Deep Contextualized Word Representations",
  booktitle = 	"Proceedings of the 2018 Conference of the North American Chapter of the      Association for Computational Linguistics: Human Language Technologies,      Volume 1 (Long Papers)    ",
  year = 	"2018",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"2227--2237",
  location = 	"New Orleans, Louisiana",
  doi = 	"10.18653/v1/N18-1202",
  url = 	"http://aclweb.org/anthology/N18-1202"
}

@article{Santoro2017,
abstract = {Relational reasoning is a central component of generally intelligent behavior, but has proven difficult for neural networks to learn. In this paper we describe how to use Relation Networks (RNs) as a simple plug-and-play module to solve problems that fundamentally hinge on relational reasoning. We tested RN-augmented networks on three tasks: visual question answering using a challenging dataset called CLEVR, on which we achieve state-of-the-art, super-human performance; text-based question answering using the bAbI suite of tasks; and complex reasoning about dynamic physical systems. Then, using a curated dataset called Sort-of-CLEVR we show that powerful convolutional networks do not have a general capacity to solve relational questions, but can gain this capacity when augmented with RNs. Our work shows how a deep learning architecture equipped with an RN module can implicitly discover and learn to reason about entities and their relations.},
archivePrefix = {arXiv},
arxivId = {1706.01427},
author = {Santoro, Adam and Raposo, David and Barrett, David G. T. and Malinowski, Mateusz and Pascanu, Razvan and Battaglia, Peter and Lillicrap, Timothy},
doi = {10.1109/WACV.2017.108},
eprint = {1706.01427},
file = {:C$\backslash$:/Users/zlai/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Santoro et al. - 2017 - A simple neural network module for relational reasoning.pdf:pdf},
isbn = {9781509048229},
issn = {21607516},
pages = {1--16},
title = {{A simple neural network module for relational reasoning}},
url = {http://arxiv.org/abs/1706.01427},
year = {2017}
}

@article{Mascharka2018,
abstract = {Visual question answering requires high-order reasoning about an image, which is a fundamental capability needed by machine systems to follow complex directives. Recently, modular networks have been shown to be an effective framework for performing visual reasoning tasks. While modular networks were initially designed with a degree of model transparency, their performance on complex visual reasoning benchmarks was lacking. Current state-of-the-art approaches do not provide an effective mechanism for understanding the reasoning process. In this paper, we close the performance gap between interpretable models and state-of-the-art visual reasoning methods. We propose a set of visual-reasoning primitives which, when composed, manifest as a model capable of performing complex reasoning tasks in an explicitly-interpretable manner. The fidelity and interpretability of the primitives' outputs enable an unparalleled ability to diagnose the strengths and weaknesses of the resulting model. Critically, we show that these primitives are highly performant, achieving state-of-the-art accuracy of 99.1{\%} on the CLEVR dataset. We also show that our model is able to effectively learn generalized representations when provided a small amount of data containing novel object attributes. Using the CoGenT generalization task, we show more than a 20 percentage point improvement over the current state of the art.},
archivePrefix = {arXiv},
arxivId = {1803.05268},
author = {Mascharka, David and Tran, Philip and Soklaski, Ryan and Majumdar, Arjun},
doi = {10.1109/CVPR.2018.00519},
eprint = {1803.05268},
file = {:C$\backslash$:/Users/zlai/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mascharka et al. - 2018 - Transparency by Design Closing the Gap Between Performance and Interpretability in Visual Reasoning.pdf:pdf},
title = {{Transparency by Design: Closing the Gap Between Performance and Interpretability in Visual Reasoning}},
url = {http://arxiv.org/abs/1803.05268},
year = {2018}
}

@article{Weston2015,
abstract = {One long-term goal of machine learning research is to produce methods that are applicable to reasoning and natural language, in particular building an intelligent dialogue agent. To measure progress towards that goal, we argue for the usefulness of a set of proxy tasks that evaluate reading comprehension via question answering. Our tasks measure understanding in several ways: whether a system is able to answer questions via chaining facts, simple induction, deduction and many more. The tasks are designed to be prerequisites for any system that aims to be capable of conversing with a human. We believe many existing learning systems can currently not solve them, and hence our aim is to classify these tasks into skill sets, so that researchers can identify (and then rectify) the failings of their systems. We also extend and improve the recently introduced Memory Networks model, and show it is able to solve some, but not all, of the tasks.},
archivePrefix = {arXiv},
arxivId = {1502.05698},
author = {Weston, Jason and Bordes, Antoine and Chopra, Sumit and Rush, Alexander M. and van Merri{\"{e}}nboer, Bart and Joulin, Armand and Mikolov, Tomas},
doi = {10.1016/j.jpowsour.2014.09.131},
eprint = {1502.05698},
file = {:C$\backslash$:/Users/zlai/Desktop/PDFs/papers/1502.05698.pdf:pdf},
isbn = {1502.05698},
issn = {03787753},
title = {{Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks}},
url = {http://arxiv.org/abs/1502.05698},
year = {2015}
}

@article{Cer2018,
abstract = {We present models for encoding sentences into embedding vectors that specifically target transfer learning to other NLP tasks. The models are efficient and result in accurate performance on diverse transfer tasks. Two variants of the encoding models allow for trade-offs between accuracy and compute resources. For both variants, we investigate and report the relationship between model complexity, resource consumption, the availability of transfer task training data, and task performance. Comparisons are made with baselines that use word level transfer learning via pretrained word embeddings as well as baselines do not use any transfer learning. We find that transfer learning using sentence embeddings tends to outperform word level transfer. With transfer learning via sentence embeddings, we observe surprisingly good performance with minimal amounts of supervised training data for a transfer task. We obtain encouraging results on Word Embedding Association Tests (WEAT) targeted at detecting model bias. Our pre-trained sentence encoding models are made freely available for download and on TF Hub.},
archivePrefix = {arXiv},
arxivId = {1803.11175},
author = {Cer, Daniel and Yang, Yinfei and Kong, Sheng-yi and Hua, Nan and Limtiaco, Nicole and John, Rhomni St. and Constant, Noah and Guajardo-Cespedes, Mario and Yuan, Steve and Tar, Chris and Sung, Yun-Hsuan and Strope, Brian and Kurzweil, Ray},
doi = {10.1007/s11423-014-9358-1},
eprint = {1803.11175},
file = {:C$\backslash$:/Users/zlai/Desktop/PDFs/papers/1803.11175.pdf:pdf},
issn = {1042-1629},
title = {{Universal Sentence Encoder}},
url = {http://arxiv.org/abs/1803.11175},
year = {2018}
}

@article{Johnson2017,
abstract = {Existing methods for visual reasoning attempt to directly map inputs to outputs using black-box architectures without explicitly modeling the underlying reasoning processes. As a result, these black-box models often learn to exploit biases in the data rather than learning to perform visual reasoning. Inspired by module networks, this paper proposes a model for visual reasoning that consists of a program generator that constructs an explicit representation of the reasoning process to be performed, and an execution engine that executes the resulting program to produce an answer. Both the program generator and the execution engine are implemented by neural networks, and are trained using a combination of backpropagation and REINFORCE. Using the CLEVR benchmark for visual reasoning, we show that our model significantly outperforms strong baselines and generalizes better in a variety of settings.},
archivePrefix = {arXiv},
arxivId = {1705.03633},
author = {Johnson, Justin and Hariharan, Bharath and Maaten, Laurens Van Der and Hoffman, Judy and Fei-Fei, Li and Zitnick, C. Lawrence and Girshick, Ross},
doi = {10.1109/ICCV.2017.325},
eprint = {1705.03633},
file = {:C$\backslash$:/Users/zlai/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Johnson et al. - 2017 - Inferring and Executing Programs for Visual Reasoning.pdf:pdf},
isbn = {9781538610329},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {3008--3017},
title = {{Inferring and Executing Programs for Visual Reasoning}},
volume = {2017-Octob},
year = {2017}
}
@inproceedings{pennington2014glove,
  author = {Jeffrey Pennington and Richard Socher and Christopher D. Manning},
  booktitle = {Empirical Methods in Natural Language Processing (EMNLP)},
  title = {GloVe: Global Vectors for Word Representation},
  year = {2014},
  pages = {1532--1543},
  url = {http://www.aclweb.org/anthology/D14-1162},
}


@article{word2vec,
  added-at = {2013-10-23T23:02:12.000+0200},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  biburl = {https://www.bibsonomy.org/bibtex/29665b85e8756834ac29fcbd2c6ad0837/wool},
  ee = {http://arxiv.org/abs/1301.3781},
  interhash = {e92df552b17e9f952226a893b84ad739},
  intrahash = {9665b85e8756834ac29fcbd2c6ad0837},
  journal = {CoRR},
  keywords = {nlp},
  timestamp = {2013-10-23T23:02:12.000+0200},
  title = {Efficient Estimation of Word Representations in Vector Space},
  url = {http://dblp.uni-trier.de/db/journals/corr/corr1301.html#abs-1301-3781},
  volume = {abs/1301.3781},
  year = 2013
}



@article{Rae2016,
abstract = {Neural networks augmented with external memory have the ability to learn algorithmic solutions to complex tasks. These models appear promising for applications such as language modeling and machine translation. However, they scale poorly in both space and time as the amount of memory grows --- limiting their applicability to real-world domains. Here, we present an end-to-end differentiable memory access scheme, which we call Sparse Access Memory (SAM), that retains the representational power of the original approaches whilst training efficiently with very large memories. We show that SAM achieves asymptotic lower bounds in space and time complexity, and find that an implementation runs {\$}1,\backslash!000\backslashtimes{\$} faster and with {\$}3,\backslash!000\backslashtimes{\$} less physical memory than non-sparse models. SAM learns with comparable data efficiency to existing models on a range of synthetic tasks and one-shot Omniglot character recognition, and can scale to tasks requiring {\$}100,\backslash!000{\$}s of time steps and memories. As well, we show how our approach can be adapted for models that maintain temporal associations between memories, as with the recently introduced Differentiable Neural Computer.},
archivePrefix = {arXiv},
arxivId = {1610.09027},
author = {Rae, Jack W and Hunt, Jonathan J and Harley, Tim and Danihelka, Ivo and Senior, Andrew and Wayne, Greg and Graves, Alex and Lillicrap, Timothy P},
doi = {10.1016/j.molimm.2007.07.004},
eprint = {1610.09027},
file = {:C$\backslash$:/Users/zlai/Desktop/PDFs/papers/1610.09027.pdf:pdf},
isbn = {1527-3296 (Electronic)$\backslash$r0196-6553 (Linking)},
issn = {10495258},
number = {Nips},
pmid = {19328363},
title = {{Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes}},
url = {http://arxiv.org/abs/1610.09027},
year = {2016}
}

@article{Graves2016,
abstract = {Modern computers separate computation and memory. Computation is performed by a processor, which can use an addressable memory to bring operands in and out of play. This confers two important benefits: the use of extensible storage to write new information and the ability to treat the contents of memory as variables. Variables are critical to algorithm generality: to perform the same procedure on one datum or another, an algorithm merely has to change the address it reads from. In contrast to computers, the computational and memory resources of artificial neural networks are mixed together in the network weights and neuron activity. This is a major liability: as the memory demands of a task increase, these networks cannot allocate new storage dynam-ically, nor easily learn algorithms that act independently of the values realized by the task variables. Although recent breakthroughs demonstrate that neural networks are remarkably adept at sensory processing 1 , sequence learning 2,3 and reinforcement learning 4 , cognitive scientists and neuroscientists have argued that neural networks are limited in their ability to represent variables and data structures 5–9 , and to store data over long timescales without interference 10,11 . We aim to combine the advantages of neu-ral and computational processing by providing a neural network with read–write access to external memory. The access is narrowly focused, minimizing interference among memoranda and enabling long-term storage 12,13 . The whole system is differentiable, and can therefore be trained end-to-end with gradient descent, allowing the network to learn how to operate and organize the memory in a goal-directed manner.},
archivePrefix = {arXiv},
arxivId = {arXiv:1410.5401v2},
author = {Graves, Alex and Wayne, Greg and Reynolds, Malcolm and Harley, Tim and Danihelka, Ivo and Grabska-Barwi{\'{n}}ska, Agnieszka and Colmenarejo, Sergio G{\'{o}}mez and Grefenstette, Edward and Ramalho, Tiago and Agapiou, John and Badia, Adri{\`{a}} Puigdom{\`{e}}nech and Hermann, Karl Moritz and Zwols, Yori and Ostrovski, Georg and Cain, Adam and King, Helen and Summerfield, Christopher and Blunsom, Phil and Kavukcuoglu, Koray and Hassabis, Demis},
doi = {10.1038/nature20101},
eprint = {arXiv:1410.5401v2},
file = {:C$\backslash$:/Users/zlai/Desktop/PDFs/papers/nature20101.pdf:pdf},
isbn = {0896-6273},
issn = {14764687},
journal = {Nature},
number = {7626},
pages = {471--476},
pmid = {26774160},
publisher = {Nature Publishing Group},
title = {{Hybrid computing using a neural network with dynamic external memory}},
url = {http://dx.doi.org/10.1038/nature20101},
volume = {538},
year = {2016}
}

@article{Sukhbaatar2015,
abstract = {We introduce a neural network with a recurrent attention model over a possibly large external memory. The architecture is a form of Memory Network (Weston et al., 2015) but unlike the model in that work, it is trained end-to-end, and hence requires significantly less supervision during training, making it more generally applicable in realistic settings. It can also be seen as an extension of RNNsearch to the case where multiple computational steps (hops) are performed per output symbol. The flexibility of the model allows us to apply it to tasks as diverse as (synthetic) question answering and to language modeling. For the former our approach is competitive with Memory Networks, but with less supervision. For the latter, on the Penn TreeBank and Text8 datasets our approach demonstrates comparable performance to RNNs and LSTMs. In both cases we show that the key concept of multiple computational hops yields improved results.},
archivePrefix = {arXiv},
arxivId = {1503.08895},
author = {Sukhbaatar, Sainbayar and Szlam, Arthur and Weston, Jason and Fergus, Rob},
doi = {v5},
eprint = {1503.08895},
file = {:C$\backslash$:/Users/zlai/Desktop/PDFs/papers/1503.08895.pdf:pdf},
isbn = {1551-6709},
issn = {10495258},
pages = {1--11},
pmid = {9377276},
title = {{End-To-End Memory Networks}},
url = {http://arxiv.org/abs/1503.08895},
year = {2015}
}


@article{Weston2015a,
abstract = {We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs.},
archivePrefix = {arXiv},
arxivId = {arXiv:1410.3916v11},
author = {Weston, Jason and Chopra, Sumit and Bordes, Antoine},
doi = {v0},
eprint = {arXiv:1410.3916v11},
file = {:C$\backslash$:/Users/zlai/Desktop/PDFs/papers/1410.3916.pdf:pdf},
isbn = {9781424469178},
issn = {1098-7576},
pages = {1--15},
pmid = {9377276},
title = {{Memory Networks}},
url = {https://arxiv.org/pdf/1410.3916.pdf},
year = {2015}
}
