Automatically generated by Mendeley Desktop 1.17.13
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Bottou2011,
abstract = {A plausible definition of "reasoning" could be "algebraically manipulating previously acquired knowledge in order to answer a new question". This definition covers first-order logical inference or probabilistic inference. It also includes much simpler manipulations commonly used to build large learning systems. For instance, we can build an optical character recognition system by first training a character segmenter, an isolated character recognizer, and a language model, using appropriate labeled training sets. Adequately concatenating these modules and fine tuning the resulting system can be viewed as an algebraic operation in a space of models. The resulting model answers a new question, that is, converting the image of a text page into a computer readable text. This observation suggests a conceptual continuity between algebraically rich inference systems, such as logical or probabilistic inference, and simple manipulations, such as the mere concatenation of trainable learning systems. Therefore, instead of trying to bridge the gap between machine learning systems and sophisticated "all-purpose" inference mechanisms, we can instead algebraically enrich the set of manipulations applicable to training systems, and build reasoning capabilities from the ground up.},
archivePrefix = {arXiv},
arxivId = {1102.1808},
author = {Bottou, Leon},
doi = {10.1007/s10994-013-5335-x},
eprint = {1102.1808},
file = {:C$\backslash$:/Users/zlai/Desktop/PDFs/papers/1102.1808.pdf:pdf},
isbn = {0885-6125},
issn = {0885-6125},
journal = {Arxiv preprint arXiv11021808},
pages = {15},
title = {{From Machine Learning to Machine Reasoning}},
url = {http://arxiv.org/abs/1102.1808},
year = {2011}
}

@article{Santoro2017,
abstract = {Relational reasoning is a central component of generally intelligent behavior, but has proven difficult for neural networks to learn. In this paper we describe how to use Relation Networks (RNs) as a simple plug-and-play module to solve problems that fundamentally hinge on relational reasoning. We tested RN-augmented networks on three tasks: visual question answering using a challenging dataset called CLEVR, on which we achieve state-of-the-art, super-human performance; text-based question answering using the bAbI suite of tasks; and complex reasoning about dynamic physical systems. Then, using a curated dataset called Sort-of-CLEVR we show that powerful convolutional networks do not have a general capacity to solve relational questions, but can gain this capacity when augmented with RNs. Our work shows how a deep learning architecture equipped with an RN module can implicitly discover and learn to reason about entities and their relations.},
archivePrefix = {arXiv},
arxivId = {1706.01427},
author = {Santoro, Adam and Raposo, David and Barrett, David G. T. and Malinowski, Mateusz and Pascanu, Razvan and Battaglia, Peter and Lillicrap, Timothy},
doi = {10.1109/WACV.2017.108},
eprint = {1706.01427},
file = {:C$\backslash$:/Users/zlai/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Santoro et al. - 2017 - A simple neural network module for relational reasoning.pdf:pdf},
isbn = {9781509048229},
issn = {21607516},
pages = {1--16},
title = {{A simple neural network module for relational reasoning}},
url = {http://arxiv.org/abs/1706.01427},
year = {2017}
}

@article{Mascharka2018,
abstract = {Visual question answering requires high-order reasoning about an image, which is a fundamental capability needed by machine systems to follow complex directives. Recently, modular networks have been shown to be an effective framework for performing visual reasoning tasks. While modular networks were initially designed with a degree of model transparency, their performance on complex visual reasoning benchmarks was lacking. Current state-of-the-art approaches do not provide an effective mechanism for understanding the reasoning process. In this paper, we close the performance gap between interpretable models and state-of-the-art visual reasoning methods. We propose a set of visual-reasoning primitives which, when composed, manifest as a model capable of performing complex reasoning tasks in an explicitly-interpretable manner. The fidelity and interpretability of the primitives' outputs enable an unparalleled ability to diagnose the strengths and weaknesses of the resulting model. Critically, we show that these primitives are highly performant, achieving state-of-the-art accuracy of 99.1{\%} on the CLEVR dataset. We also show that our model is able to effectively learn generalized representations when provided a small amount of data containing novel object attributes. Using the CoGenT generalization task, we show more than a 20 percentage point improvement over the current state of the art.},
archivePrefix = {arXiv},
arxivId = {1803.05268},
author = {Mascharka, David and Tran, Philip and Soklaski, Ryan and Majumdar, Arjun},
doi = {10.1109/CVPR.2018.00519},
eprint = {1803.05268},
file = {:C$\backslash$:/Users/zlai/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mascharka et al. - 2018 - Transparency by Design Closing the Gap Between Performance and Interpretability in Visual Reasoning.pdf:pdf},
title = {{Transparency by Design: Closing the Gap Between Performance and Interpretability in Visual Reasoning}},
url = {http://arxiv.org/abs/1803.05268},
year = {2018}
}

